{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulSerin/Big-Data-Framework/blob/main/BDF_12_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9155d9-6190-4885-fd8e-20704f873e09"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connecting to r2u.stat.illinois.edu (19\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2024-11-29 14:15:55--  http://apache.osuosl.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 140.211.166.134, 64.50.233.100, 64.50.236.52, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|140.211.166.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400864419 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.3-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.3-bin-had 100%[===================>] 382.29M   109MB/s    in 4.2s    \n",
            "\n",
            "2024-11-29 14:15:59 (90.4 MB/s) - ‘spark-3.5.3-bin-hadoop3.tgz’ saved [400864419/400864419]\n",
            "\n",
            "spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0f491c-77c7-42cc-f9f5-4741ea3fec62"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Big Data Framework/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/Big Data Framework/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8170c0-78c5-4b07-ee3b-e6b4b33f5d9b"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "PySpark version 3.5.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f21ad0-bfa3-46a5-eb2e-6d584d1f2ffb"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 12 - Exercises. Final assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za4qzjHXyxcn"
      },
      "source": [
        "## Exercise 12.1\n",
        "\n",
        "Let us extract information from the cite75_99.txt and apat63_99.txt files. Write a script that performs the following operations:\n",
        "\n",
        "1. From the cite75_99.txt file, obtain the number of citations received by each patent. You must produce a DataFrame with the following format:\n",
        "\n",
        "| PatentNum | ncitations |\n",
        "|-----------|------------|\n",
        "| 3060453   |    3       |\n",
        "| 3390168   |    6       |\n",
        "| 3626542   |   18       |\n",
        "| 3611507   |    5       |\n",
        "| 3000113   |    4       |\n",
        "\n",
        "\n",
        "2. From the apat63_99.txt file, create a DataFrame to show the patent number, its country and the patent year, discarding the rest of fields in the file. The DataFrame produced must have the following format:\n",
        "\n",
        "|PatentNum |country|Year |\n",
        "|----------|-------|-----|\n",
        "| 3070801  | BE    | 1963|\n",
        "| 3070802  | US    | 1963|\n",
        "| 3070803  | US    | 1963|\n",
        "| 3070804  | US    | 1963|\n",
        "| 3070805  | US    | 1963|\n",
        "\n",
        "\n",
        "**Requirements**\n",
        "\n",
        " - Both DataFrames must be stored in Parquet format with gzip compression. Check the number of partitions of each DataFrame and the number of files gererated.\n",
        "\n",
        " - It is **strongly advised** to copy the files from your Drive to a temporal directory in the notebook virtual machine and unzip them there. This will reduce the execution times. See the cell below:\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV_M6xMlB9hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e7686d-368c-4949-f0ac-7093c3cbd194"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the cite75_99.txt file as a text DataFrame\n",
        "cite_path = os.environ[\"DRIVE_DATA\"] + \"cite75_99.txt\"\n",
        "cite_df = spark.read.text(cite_path)\n",
        "\n",
        "# Display raw data\n",
        "print(\"Raw data:\")\n",
        "cite_df.show(5, truncate=False)\n",
        "\n",
        "# Step 1: Filter out header and split into Citing and Cited columns\n",
        "cite_split_df = (\n",
        "    cite_df.filter(~F.col(\"value\").contains(\"CITING\"))  # Remove the header\n",
        "    .withColumn(\"Citing\", F.split(F.col(\"value\"), \",\").getItem(0).cast(\"long\"))\n",
        "    .withColumn(\"Cited\", F.split(F.col(\"value\"), \",\").getItem(1).cast(\"long\"))\n",
        "    .drop(\"value\")\n",
        ")\n",
        "\n",
        "# Display split data\n",
        "print(\"Split data:\")\n",
        "cite_split_df.show(5, truncate=False)\n",
        "\n",
        "cite_split_df.createOrReplaceTempView(\"cite_data\")\n",
        "\n",
        "# Use SQL to compute citation counts\n",
        "citations_sql_df = spark.sql(\"\"\"\n",
        "    SELECT Cited AS PatentNum, COUNT(*) AS ncitations\n",
        "    FROM cite_data\n",
        "    GROUP BY Cited\n",
        "    ORDER BY Cited\n",
        "\"\"\")\n",
        "\n",
        "# Show SQL results\n",
        "print(\"Citations count (SQL):\")\n",
        "citations_sql_df.show(5, truncate=False)\n",
        "\n",
        "# Save SQL results as Parquet with gzip compression\n",
        "output_path_citations_sql = \"/tmp/data/citations_count_sql.parquet\"\n",
        "citations_sql_df.write.parquet(output_path_citations_sql, compression=\"gzip\", mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved Parquet files\n",
        "print(f\"Number of partitions (SQL): {citations_sql_df.rdd.getNumPartitions()}\")\n",
        "!ls -lh {output_path_citations_sql}\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data:\n",
            "+----------------+\n",
            "|value           |\n",
            "+----------------+\n",
            "|\"CITING\",\"CITED\"|\n",
            "|3858241,956203  |\n",
            "|3858241,1324234 |\n",
            "|3858241,3398406 |\n",
            "|3858241,3557384 |\n",
            "+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Split data:\n",
            "+-------+-------+\n",
            "|Citing |Cited  |\n",
            "+-------+-------+\n",
            "|3858241|956203 |\n",
            "|3858241|1324234|\n",
            "|3858241|3398406|\n",
            "|3858241|3557384|\n",
            "|3858241|3634889|\n",
            "+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Citations count (SQL):\n",
            "+---------+----------+\n",
            "|PatentNum|ncitations|\n",
            "+---------+----------+\n",
            "|1        |2         |\n",
            "|13       |2         |\n",
            "|24       |1         |\n",
            "|29       |1         |\n",
            "|31       |2         |\n",
            "+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Number of partitions (SQL): 2\n",
            "total 6.5M\n",
            "-rw-r--r-- 1 root root 3.2M Nov 29 14:20 part-00000-2fdbdece-3d1e-4d6d-81a5-ca8e257e18d0-c000.gz.parquet\n",
            "-rw-r--r-- 1 root root 3.3M Nov 29 14:20 part-00001-2fdbdece-3d1e-4d6d-81a5-ca8e257e18d0-c000.gz.parquet\n",
            "-rw-r--r-- 1 root root    0 Nov 29 14:20 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the apat63_99.txt file\n",
        "apat_path = os.environ[\"DRIVE_DATA\"] + \"apat63_99.txt\"\n",
        "\n",
        "# Specify the correct delimiter and header\n",
        "apat_df = spark.read.csv(apat_path, sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the raw DataFrame to confirm correct loading\n",
        "apat_df.show(5, truncate=False)\n",
        "\n",
        "# Select relevant columns and rename them\n",
        "apat_selected_df = (\n",
        "    apat_df.select(F.col(\"PATENT\").alias(\"PatentNum\"),\n",
        "                   F.col(\"COUNTRY\").alias(\"country\"),\n",
        "                   F.col(\"GYEAR\").alias(\"Year\"))\n",
        ")\n",
        "\n",
        "# Show the selected columns\n",
        "apat_selected_df.show(5, truncate=False)\n",
        "\n",
        "# Save as Parquet with gzip compression\n",
        "output_path_apat = \"/tmp/data/apat_selected.parquet\"\n",
        "apat_selected_df.write.parquet(output_path_apat, compression=\"gzip\", mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved Parquet files\n",
        "print(f\"Number of partitions: {apat_selected_df.rdd.getNumPartitions()}\")\n",
        "!ls -lh {output_path_apat}"
      ],
      "metadata": {
        "id": "8qnA4a7aSkyK",
        "outputId": "f2a4216d-dfec-41e0-ceac-75b4df86545b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
            "|PATENT |GYEAR|GDATE|APPYEAR|COUNTRY|POSTATE|ASSIGNEE|ASSCODE|CLAIMS|NCLASS|CAT|SUBCAT|CMADE|CRECEIVE|RATIOCIT|GENERAL|ORIGINAL|FWDAPLAG|BCKGTLAG|SELFCTUB|SELFCTLB|SECDUPBD|SECDLWBD|\n",
            "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
            "|3070801|1963 |1096 |NULL   |BE     |NULL   |NULL    |1      |NULL  |269   |6  |69    |NULL |1       |NULL    |0.0    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "|3070802|1963 |1096 |NULL   |US     |TX     |NULL    |1      |NULL  |2     |6  |63    |NULL |0       |NULL    |NULL   |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "|3070803|1963 |1096 |NULL   |US     |IL     |NULL    |1      |NULL  |2     |6  |63    |NULL |9       |NULL    |0.3704 |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "|3070804|1963 |1096 |NULL   |US     |OH     |NULL    |1      |NULL  |2     |6  |63    |NULL |3       |NULL    |0.6667 |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "|3070805|1963 |1096 |NULL   |US     |CA     |NULL    |1      |NULL  |2     |6  |63    |NULL |1       |NULL    |0.0    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+-------+----+\n",
            "|PatentNum|country|Year|\n",
            "+---------+-------+----+\n",
            "|3070801  |BE     |1963|\n",
            "|3070802  |US     |1963|\n",
            "|3070803  |US     |1963|\n",
            "|3070804  |US     |1963|\n",
            "|3070805  |US     |1963|\n",
            "+---------+-------+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Number of partitions: 2\n",
            "total 5.1M\n",
            "-rw-r--r-- 1 root root 2.8M Nov 29 14:25 part-00000-8fe5bb74-4feb-4813-b6a4-ce258a98c07f-c000.gz.parquet\n",
            "-rw-r--r-- 1 root root 2.4M Nov 29 14:25 part-00001-8fe5bb74-4feb-4813-b6a4-ce258a98c07f-c000.gz.parquet\n",
            "-rw-r--r-- 1 root root    0 Nov 29 14:25 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF1_LXZEnzMn"
      },
      "source": [
        "## Exercise 12.2\n",
        "\n",
        "Write a code that, from the Parquet files created in the previous exercise, obtains for each country and for each year: the total number of patents, the total number of citations from those patents, the average number of citations and the maximum number of citations. Compute only those values in which there are any values in both files (*inner join*). In addition, each country must show its whole name, obtained from the *country_codes.txt* file. The final DataFrame must look like this one:\n",
        "\n",
        "\n",
        "|Country            |Year|PatentsNum |TotalCitations|AvgCitations      |MaxCitations|\n",
        "|-------------------|----|-----------|--------------|------------------|------------|\n",
        "|Algeria            |1963|2          |7             |3.5               |4           |\n",
        "|Algeria            |1968|1          |2             |2.0               |2           |\n",
        "|Algeria            |1970|1          |2             |2.0               |2           |\n",
        "|Algeria            |1972|1          |1             |1.0               |1           |\n",
        "|Algeria            |1977|1          |2             |2.0               |2           |\n",
        "|Andorra            |1987|1          |3             |3.0               |3           |\n",
        "|Andorra            |1993|1          |1             |1.0               |1           |\n",
        "|Andorra            |1998|1          |1             |1.0               |1           |\n",
        "|Antigua and Barbuda|1978|1          |6             |6.0               |6           |\n",
        "|Antigua and Barbuda|1979|1          |14            |14.0              |14          |\n",
        "|Antigua and Barbuda|1991|1          |8             |8.0               |8           |\n",
        "|Antigua and Barbuda|1994|1          |19            |19.0              |19          |\n",
        "|Antigua and Barbuda|1995|2          |12            |6.0               |11          |\n",
        "|Antigua and Barbuda|1996|2          |3             |1.5               |2           |\n",
        "|Argentina          |1963|14         |35            |2.5               |7           |\n",
        "|Argentina          |1964|20         |60            |3.0               |8           |\n",
        "|Argentina          |1965|10         |35            |3.5               |10          |\n",
        "|Argentina          |1966|16         |44            |2.75              |9           |\n",
        "|Argentina          |1967|13         |60            |4.615384615384615 |14          |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The output DataFrame must be saved in a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the Parquet files\n",
        "citations_df = spark.read.parquet(\"/tmp/data/citations_count.parquet\")\n",
        "patents_df = spark.read.parquet(\"/tmp/data/apat_selected.parquet\")\n",
        "\n",
        "# Show the loaded data\n",
        "print(\"Citations DataFrame:\")\n",
        "citations_df.show(5, truncate=False)\n",
        "\n",
        "print(\"Patents DataFrame:\")\n",
        "patents_df.show(5, truncate=False)\n",
        "\n",
        "# Inner join between patents and citations using PatentNum\n",
        "joined_df = patents_df.join(\n",
        "    citations_df,\n",
        "    patents_df.PatentNum == citations_df.Cited,\n",
        "    how=\"inner\"\n",
        ").drop(\"Cited\")  # Drop redundant column\n",
        "\n",
        "# Group and aggregate\n",
        "aggregated_df = joined_df.groupBy(\"country\", \"Year\").agg(\n",
        "    F.count(\"PatentNum\").alias(\"PatentsNum\"),\n",
        "    F.sum(\"ncitations\").alias(\"TotalCitations\"),\n",
        "    F.avg(\"ncitations\").alias(\"AvgCitations\"),\n",
        "    F.max(\"ncitations\").alias(\"MaxCitations\")\n",
        ")\n",
        "\n",
        "# Load the country codes file\n",
        "country_codes_path = os.environ[\"DRIVE_DATA\"] + \"country_codes.txt\"\n",
        "country_codes_df = spark.read.csv(country_codes_path, sep=\",\", header=True)\n",
        "\n",
        "# Verify the schema of country_codes_df\n",
        "print(\"Country Codes DataFrame:\")\n",
        "country_codes_df.show(5, truncate=False)\n",
        "\n",
        "# Correct column names based on the actual file structure\n",
        "country_codes_df = country_codes_df.withColumnRenamed(\"country_code\", \"code\").withColumnRenamed(\"country_name\", \"name\")\n",
        "\n",
        "# Join to replace country codes with full names\n",
        "final_df = aggregated_df.join(\n",
        "    country_codes_df,\n",
        "    aggregated_df.country == country_codes_df.code,\n",
        "    how=\"inner\"\n",
        ").select(\n",
        "    F.col(\"name\").alias(\"Country\"),\n",
        "    \"Year\",\n",
        "    \"PatentsNum\",\n",
        "    \"TotalCitations\",\n",
        "    \"AvgCitations\",\n",
        "    \"MaxCitations\"\n",
        ")\n",
        "\n",
        "# Show the final DataFrame\n",
        "print(\"Final DataFrame:\")\n",
        "final_df.show(10, truncate=False)\n",
        "\n",
        "# Save the final DataFrame as a single CSV file\n",
        "output_path = \"/tmp/data/final_patents_citations.csv\"\n",
        "final_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved CSV file\n",
        "print(\"Saved CSV File:\")\n",
        "!ls -lh {output_path}\n"
      ],
      "metadata": {
        "id": "dxyt7LAWUTQ7",
        "outputId": "53d8ee9b-40fe-4934-fb99-2216977ddc47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citations DataFrame:\n",
            "+-------+----------+\n",
            "|Cited  |ncitations|\n",
            "+-------+----------+\n",
            "|3858272|3         |\n",
            "|3858273|3         |\n",
            "|3858276|6         |\n",
            "|3858277|1         |\n",
            "|3858278|3         |\n",
            "+-------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Patents DataFrame:\n",
            "+---------+-------+----+\n",
            "|PatentNum|country|Year|\n",
            "+---------+-------+----+\n",
            "|3070801  |BE     |1963|\n",
            "|3070802  |US     |1963|\n",
            "|3070803  |US     |1963|\n",
            "|3070804  |US     |1963|\n",
            "|3070805  |US     |1963|\n",
            "+---------+-------+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Country Codes DataFrame:\n",
            "+------------------+\n",
            "|AF\\tAfghanistan   |\n",
            "+------------------+\n",
            "|AX\\tAland Islands |\n",
            "|AL\\tAlbania       |\n",
            "|DZ\\tAlgeria       |\n",
            "|AS\\tAmerican Samoa|\n",
            "|AD\\tAndorra       |\n",
            "+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'code'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-a6103c8c207f>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m final_df = aggregated_df.join(\n\u001b[1;32m     42\u001b[0m     \u001b[0mcountry_codes_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0maggregated_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountry\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcountry_codes_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3127\u001b[0m         \"\"\"\n\u001b[1;32m   3128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3130\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'code'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT3WU-1IwOPD"
      },
      "source": [
        "## Exercise 12.3\n",
        "\n",
        "From the apat63_99.txt file, obtain the number of patents per country and year **using RDDs** (do not use DataFrames). The resulting RDD must be a key/value RDD in which the key is a country and the value a list of tuples. Each tuple will be composed of a year and the number of patents of the country during that year. In addition, the resulting RDD must be sorted by  the country code and, for each country, values must be sorted by year.\n",
        "\n",
        "Example of output key/value entry:\n",
        "\n",
        "    (u'PA', [(u'1963', 2), (u'1964', 2), (u'1965', 1), (u'1966', 1), (u'1970', 1), (u'1971', 1), (u'1972', 6), (u'1974', 3), (u'1975', 5), (u'1976', 3), (u'1977', 2), (u'1978', 2), (u'1980', 2), (u'1982', 1), (u'1983', 1), (u'1985', 2), (u'1986', 1), (u'1987', 2), (u'1988', 1), (u'1990', 1), (u'1991', 2), (u'1993', 1), (u'1995', 1), (u'1996', 1), (u'1999', 1)])\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- You must remove the double quotation marks from the country code.\n",
        "- Use 8 partitions to read the apat63_99.txt.bz2 file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0LiWoc4VQdh"
      },
      "source": [
        "## Exercise 12.4\n",
        "\n",
        "From the Parquet files created in Exercise 12.1, create a DataFrame that gives the patent or patents with the higher number of citations per country and year, as well as the average of the number of citations per country and year, and the difference between the maximum and the average values. The resulting DataFrame should look like this:\n",
        "\n",
        "\n",
        "|Country|Year|PatentNum|max  |average       |diff              |\n",
        "|-------|----|---------|-----|--------------|------------------|\n",
        "|AD     |1987|4688621  |3    |3.0           |0.0               |\n",
        "|AD     |1993|5193231  |1    |1.0           |0.0               |\n",
        "|AD     |1998|5765303  |1    |1.0           |0.0               |\n",
        "|AE     |1984|4482959  |5    |5.0           |0.0               |\n",
        "|AE     |1985|4554981  |14   |14.0          |0.0               |\n",
        "|AE     |1987|4663181  |3    |3.0           |0.0               |\n",
        "|AE     |1989|4805221  |7    |5.0           |2.0               |\n",
        "|AE     |1990|4909321  |2    |2.0           |0.0               |\n",
        "|AE     |1991|5004552  |3    |2.0           |1.0               |\n",
        "|AE     |1992|5104556  |4    |4.0           |0.0               |\n",
        "|AE     |1993|5181569  |8    |8.0           |0.0               |\n",
        "|AE     |1996|5580125  |1    |1.0           |0.0               |\n",
        "|AG     |1978|4126850  |6    |6.0           |0.0               |\n",
        "|AG     |1979|4172981  |14   |14.0          |0.0               |\n",
        "|AG     |1991|5013035  |8    |8.0           |0.0               |\n",
        "|AG     |1994|5345071  |19   |19.0          |0.0               |\n",
        "|AG     |1995|5457307  |11   |6.0           |5.0               |\n",
        "|AG     |1996|5525786  |2    |1.5           |0.5               |\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektPyLWzaImT"
      },
      "source": [
        "## Exercise 12.5\n",
        "\n",
        "From the Parquet file with the (PatentNum,Country,Year) information from Exercise 12.1, create a DataFrame that shows the number of patents associated to each country per decade (understanding as a *decade* the years from 0 to 9; e.g. from 1970 to 1979). In addition, the DataFrame must show the increase or decrease of the number of patents per country and decade with respect to the previous decade. The resulting DataFrame must look like this:\n",
        "\n",
        "|Country|Decade|PatentsNum|Diff|\n",
        "|-------|------|----------|----|\n",
        "|AD     |1980  |1         |0   |\n",
        "|AD     |1990  |5         |4   |\n",
        "|AE     |1980  |7         |0   |\n",
        "|AE     |1990  |11        |4   |\n",
        "|AG     |1970  |2         |0   |\n",
        "|AG     |1990  |7         |5   |\n",
        "|AI     |1990  |1         |0   |\n",
        "|AL     |1990  |1         |0   |\n",
        "|AM     |1990  |2         |0   |\n",
        "|AN     |1970  |1         |0   |\n",
        "|AN     |1980  |2         |1   |\n",
        "|AN     |1990  |5         |3   |\n",
        "|AR     |1960  |135       |0   |\n",
        "|AR     |1970  |239       |104 |\n",
        "|AR     |1980  |184       |-55 |\n",
        "|AR     |1990  |292       |108 |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression."
      ]
    }
  ]
}