{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulSerin/Big-Data-Framework/blob/main/BDF_12_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf",
        "outputId": "0c9155d9-6190-4885-fd8e-20704f873e09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connecting to r2u.stat.illinois.edu (19\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2024-11-29 14:15:55--  http://apache.osuosl.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 140.211.166.134, 64.50.233.100, 64.50.236.52, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|140.211.166.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400864419 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.3-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.3-bin-had 100%[===================>] 382.29M   109MB/s    in 4.2s    \n",
            "\n",
            "2024-11-29 14:15:59 (90.4 MB/s) - ‘spark-3.5.3-bin-hadoop3.tgz’ saved [400864419/400864419]\n",
            "\n",
            "spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx",
        "outputId": "9c0f491c-77c7-42cc-f9f5-4741ea3fec62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Big Data Framework/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/Big Data Framework/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN",
        "outputId": "cb8170c0-78c5-4b07-ee3b-e6b4b33f5d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "PySpark version 3.5.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT",
        "outputId": "f3f21ad0-bfa3-46a5-eb2e-6d584d1f2ffb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 12 - Exercises. Final assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za4qzjHXyxcn"
      },
      "source": [
        "## Exercise 12.1\n",
        "\n",
        "Let us extract information from the cite75_99.txt and apat63_99.txt files. Write a script that performs the following operations:\n",
        "\n",
        "1. From the cite75_99.txt file, obtain the number of citations received by each patent. You must produce a DataFrame with the following format:\n",
        "\n",
        "| PatentNum | ncitations |\n",
        "|-----------|------------|\n",
        "| 3060453   |    3       |\n",
        "| 3390168   |    6       |\n",
        "| 3626542   |   18       |\n",
        "| 3611507   |    5       |\n",
        "| 3000113   |    4       |\n",
        "\n",
        "\n",
        "2. From the apat63_99.txt file, create a DataFrame to show the patent number, its country and the patent year, discarding the rest of fields in the file. The DataFrame produced must have the following format:\n",
        "\n",
        "|PatentNum |country|Year |\n",
        "|----------|-------|-----|\n",
        "| 3070801  | BE    | 1963|\n",
        "| 3070802  | US    | 1963|\n",
        "| 3070803  | US    | 1963|\n",
        "| 3070804  | US    | 1963|\n",
        "| 3070805  | US    | 1963|\n",
        "\n",
        "\n",
        "**Requirements**\n",
        "\n",
        " - Both DataFrames must be stored in Parquet format with gzip compression. Check the number of partitions of each DataFrame and the number of files gererated.\n",
        "\n",
        " - It is **strongly advised** to copy the files from your Drive to a temporal directory in the notebook virtual machine and unzip them there. This will reduce the execution times. See the cell below:\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV_M6xMlB9hP",
        "outputId": "13a71bdb-9fee-40e6-f4b3-3942c9ef3ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the cite75_99.txt file as a text DataFrame\n",
        "cite_path = os.environ[\"DRIVE_DATA\"] + \"cite75_99.txt\"\n",
        "cite_df = spark.read.text(cite_path)\n",
        "\n",
        "# Display raw data\n",
        "print(\"Raw data:\")\n",
        "cite_df.show(5, truncate=False)\n",
        "\n",
        "# Step 1: Filter out header and split into Citing and Cited columns\n",
        "cite_split_df = (\n",
        "    cite_df.filter(~F.col(\"value\").contains(\"CITING\"))  # Remove the header\n",
        "    .withColumn(\"Citing\", F.split(F.col(\"value\"), \",\").getItem(0).cast(\"long\"))\n",
        "    .withColumn(\"Cited\", F.split(F.col(\"value\"), \",\").getItem(1).cast(\"long\"))\n",
        "    .drop(\"value\")\n",
        ")\n",
        "\n",
        "# Display split data\n",
        "print(\"Split data:\")\n",
        "cite_split_df.show(5, truncate=False)\n",
        "\n",
        "# Step 2: Group by 'Cited' and count occurrences\n",
        "citations_count_df = (\n",
        "    cite_split_df.groupBy(\"Cited\")\n",
        "    .count()\n",
        "    .withColumnRenamed(\"count\", \"ncitations\")\n",
        "    .orderBy(\"Cited\")\n",
        ")\n",
        "\n",
        "# Display citation counts\n",
        "print(\"Citations count:\")\n",
        "citations_count_df.show(5, truncate=False)\n",
        "\n",
        "# Step 3: Save as Parquet with gzip compression\n",
        "output_path_citations = \"/tmp/data/citations_count.parquet\"\n",
        "citations_count_df.write.parquet(output_path_citations, compression=\"gzip\", mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved Parquet files\n",
        "print(f\"Number of partitions: {citations_count_df.rdd.getNumPartitions()}\")\n",
        "!ls -lh {output_path_citations}\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/gdrive/My Drive/Enseignement/2024-2025/ING3/HPDA/BigDataFrameworks/data/cite75_99.txt.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-8d56d930d30a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the cite75_99.txt file as a text DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcite_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DRIVE_DATA\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cite75_99.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcite_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcite_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Display raw data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     def csv(\n",
            "\u001b[0;32m/content/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/gdrive/My Drive/Enseignement/2024-2025/ING3/HPDA/BigDataFrameworks/data/cite75_99.txt."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF1_LXZEnzMn"
      },
      "source": [
        "## Exercise 12.2\n",
        "\n",
        "Write a code that, from the Parquet files created in the previous exercise, obtains for each country and for each year: the total number of patents, the total number of citations from those patents, the average number of citations and the maximum number of citations. Compute only those values in which there are any values in both files (*inner join*). In addition, each country must show its whole name, obtained from the *country_codes.txt* file. The final DataFrame must look like this one:\n",
        "\n",
        "\n",
        "|Country            |Year|PatentsNum |TotalCitations|AvgCitations      |MaxCitations|\n",
        "|-------------------|----|-----------|--------------|------------------|------------|\n",
        "|Algeria            |1963|2          |7             |3.5               |4           |\n",
        "|Algeria            |1968|1          |2             |2.0               |2           |\n",
        "|Algeria            |1970|1          |2             |2.0               |2           |\n",
        "|Algeria            |1972|1          |1             |1.0               |1           |\n",
        "|Algeria            |1977|1          |2             |2.0               |2           |\n",
        "|Andorra            |1987|1          |3             |3.0               |3           |\n",
        "|Andorra            |1993|1          |1             |1.0               |1           |\n",
        "|Andorra            |1998|1          |1             |1.0               |1           |\n",
        "|Antigua and Barbuda|1978|1          |6             |6.0               |6           |\n",
        "|Antigua and Barbuda|1979|1          |14            |14.0              |14          |\n",
        "|Antigua and Barbuda|1991|1          |8             |8.0               |8           |\n",
        "|Antigua and Barbuda|1994|1          |19            |19.0              |19          |\n",
        "|Antigua and Barbuda|1995|2          |12            |6.0               |11          |\n",
        "|Antigua and Barbuda|1996|2          |3             |1.5               |2           |\n",
        "|Argentina          |1963|14         |35            |2.5               |7           |\n",
        "|Argentina          |1964|20         |60            |3.0               |8           |\n",
        "|Argentina          |1965|10         |35            |3.5               |10          |\n",
        "|Argentina          |1966|16         |44            |2.75              |9           |\n",
        "|Argentina          |1967|13         |60            |4.615384615384615 |14          |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The output DataFrame must be saved in a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT3WU-1IwOPD"
      },
      "source": [
        "## Exercise 12.3\n",
        "\n",
        "From the apat63_99.txt file, obtain the number of patents per country and year **using RDDs** (do not use DataFrames). The resulting RDD must be a key/value RDD in which the key is a country and the value a list of tuples. Each tuple will be composed of a year and the number of patents of the country during that year. In addition, the resulting RDD must be sorted by  the country code and, for each country, values must be sorted by year.\n",
        "\n",
        "Example of output key/value entry:\n",
        "\n",
        "    (u'PA', [(u'1963', 2), (u'1964', 2), (u'1965', 1), (u'1966', 1), (u'1970', 1), (u'1971', 1), (u'1972', 6), (u'1974', 3), (u'1975', 5), (u'1976', 3), (u'1977', 2), (u'1978', 2), (u'1980', 2), (u'1982', 1), (u'1983', 1), (u'1985', 2), (u'1986', 1), (u'1987', 2), (u'1988', 1), (u'1990', 1), (u'1991', 2), (u'1993', 1), (u'1995', 1), (u'1996', 1), (u'1999', 1)])\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- You must remove the double quotation marks from the country code.\n",
        "- Use 8 partitions to read the apat63_99.txt.bz2 file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0LiWoc4VQdh"
      },
      "source": [
        "## Exercise 12.4\n",
        "\n",
        "From the Parquet files created in Exercise 12.1, create a DataFrame that gives the patent or patents with the higher number of citations per country and year, as well as the average of the number of citations per country and year, and the difference between the maximum and the average values. The resulting DataFrame should look like this:\n",
        "\n",
        "\n",
        "|Country|Year|PatentNum|max  |average       |diff              |\n",
        "|-------|----|---------|-----|--------------|------------------|\n",
        "|AD     |1987|4688621  |3    |3.0           |0.0               |\n",
        "|AD     |1993|5193231  |1    |1.0           |0.0               |\n",
        "|AD     |1998|5765303  |1    |1.0           |0.0               |\n",
        "|AE     |1984|4482959  |5    |5.0           |0.0               |\n",
        "|AE     |1985|4554981  |14   |14.0          |0.0               |\n",
        "|AE     |1987|4663181  |3    |3.0           |0.0               |\n",
        "|AE     |1989|4805221  |7    |5.0           |2.0               |\n",
        "|AE     |1990|4909321  |2    |2.0           |0.0               |\n",
        "|AE     |1991|5004552  |3    |2.0           |1.0               |\n",
        "|AE     |1992|5104556  |4    |4.0           |0.0               |\n",
        "|AE     |1993|5181569  |8    |8.0           |0.0               |\n",
        "|AE     |1996|5580125  |1    |1.0           |0.0               |\n",
        "|AG     |1978|4126850  |6    |6.0           |0.0               |\n",
        "|AG     |1979|4172981  |14   |14.0          |0.0               |\n",
        "|AG     |1991|5013035  |8    |8.0           |0.0               |\n",
        "|AG     |1994|5345071  |19   |19.0          |0.0               |\n",
        "|AG     |1995|5457307  |11   |6.0           |5.0               |\n",
        "|AG     |1996|5525786  |2    |1.5           |0.5               |\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektPyLWzaImT"
      },
      "source": [
        "## Exercise 12.5\n",
        "\n",
        "From the Parquet file with the (PatentNum,Country,Year) information from Exercise 12.1, create a DataFrame that shows the number of patents associated to each country per decade (understanding as a *decade* the years from 0 to 9; e.g. from 1970 to 1979). In addition, the DataFrame must show the increase or decrease of the number of patents per country and decade with respect to the previous decade. The resulting DataFrame must look like this:\n",
        "\n",
        "|Country|Decade|PatentsNum|Diff|\n",
        "|-------|------|----------|----|\n",
        "|AD     |1980  |1         |0   |\n",
        "|AD     |1990  |5         |4   |\n",
        "|AE     |1980  |7         |0   |\n",
        "|AE     |1990  |11        |4   |\n",
        "|AG     |1970  |2         |0   |\n",
        "|AG     |1990  |7         |5   |\n",
        "|AI     |1990  |1         |0   |\n",
        "|AL     |1990  |1         |0   |\n",
        "|AM     |1990  |2         |0   |\n",
        "|AN     |1970  |1         |0   |\n",
        "|AN     |1980  |2         |1   |\n",
        "|AN     |1990  |5         |3   |\n",
        "|AR     |1960  |135       |0   |\n",
        "|AR     |1970  |239       |104 |\n",
        "|AR     |1980  |184       |-55 |\n",
        "|AR     |1990  |292       |108 |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression."
      ]
    }
  ]
}