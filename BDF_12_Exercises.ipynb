{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulSerin/Big-Data-Framework/blob/main/BDF_12_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9155d9-6190-4885-fd8e-20704f873e09"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Connecting to r2u.stat.illinois.edu (19\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2024-11-29 14:15:55--  http://apache.osuosl.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 140.211.166.134, 64.50.233.100, 64.50.236.52, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|140.211.166.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400864419 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.3-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.3-bin-had 100%[===================>] 382.29M   109MB/s    in 4.2s    \n",
            "\n",
            "2024-11-29 14:15:59 (90.4 MB/s) - ‘spark-3.5.3-bin-hadoop3.tgz’ saved [400864419/400864419]\n",
            "\n",
            "spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0f491c-77c7-42cc-f9f5-4741ea3fec62"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Big Data Framework/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/Big Data Framework/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8170c0-78c5-4b07-ee3b-e6b4b33f5d9b"
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "PySpark version 3.5.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f21ad0-bfa3-46a5-eb2e-6d584d1f2ffb"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 12 - Exercises. Final assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za4qzjHXyxcn"
      },
      "source": [
        "## Exercise 12.1\n",
        "\n",
        "Let us extract information from the cite75_99.txt and apat63_99.txt files. Write a script that performs the following operations:\n",
        "\n",
        "1. From the cite75_99.txt file, obtain the number of citations received by each patent. You must produce a DataFrame with the following format:\n",
        "\n",
        "| PatentNum | ncitations |\n",
        "|-----------|------------|\n",
        "| 3060453   |    3       |\n",
        "| 3390168   |    6       |\n",
        "| 3626542   |   18       |\n",
        "| 3611507   |    5       |\n",
        "| 3000113   |    4       |\n",
        "\n",
        "\n",
        "2. From the apat63_99.txt file, create a DataFrame to show the patent number, its country and the patent year, discarding the rest of fields in the file. The DataFrame produced must have the following format:\n",
        "\n",
        "|PatentNum |country|Year |\n",
        "|----------|-------|-----|\n",
        "| 3070801  | BE    | 1963|\n",
        "| 3070802  | US    | 1963|\n",
        "| 3070803  | US    | 1963|\n",
        "| 3070804  | US    | 1963|\n",
        "| 3070805  | US    | 1963|\n",
        "\n",
        "\n",
        "**Requirements**\n",
        "\n",
        " - Both DataFrames must be stored in Parquet format with gzip compression. Check the number of partitions of each DataFrame and the number of files gererated.\n",
        "\n",
        " - It is **strongly advised** to copy the files from your Drive to a temporal directory in the notebook virtual machine and unzip them there. This will reduce the execution times. See the cell below:\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV_M6xMlB9hP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e7686d-368c-4949-f0ac-7093c3cbd194"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the cite75_99.txt file as a text DataFrame\n",
        "cite_path = os.environ[\"DRIVE_DATA\"] + \"cite75_99.txt\"\n",
        "cite_df = spark.read.text(cite_path)\n",
        "\n",
        "# Display raw data\n",
        "print(\"Raw data:\")\n",
        "cite_df.show(5, truncate=False)\n",
        "\n",
        "# Step 1: Filter out header and split into Citing and Cited columns\n",
        "cite_split_df = (\n",
        "    cite_df.filter(~F.col(\"value\").contains(\"CITING\"))  # Remove the header\n",
        "    .withColumn(\"Citing\", F.split(F.col(\"value\"), \",\").getItem(0).cast(\"long\"))\n",
        "    .withColumn(\"Cited\", F.split(F.col(\"value\"), \",\").getItem(1).cast(\"long\"))\n",
        "    .drop(\"value\")\n",
        ")\n",
        "\n",
        "# Display split data\n",
        "print(\"Split data:\")\n",
        "cite_split_df.show(5, truncate=False)\n",
        "\n",
        "cite_split_df.createOrReplaceTempView(\"cite_data\")\n",
        "\n",
        "# Use SQL to compute citation counts\n",
        "citations_sql_df = spark.sql(\"\"\"\n",
        "    SELECT Cited AS PatentNum, COUNT(*) AS ncitations\n",
        "    FROM cite_data\n",
        "    GROUP BY Cited\n",
        "    ORDER BY Cited\n",
        "\"\"\")\n",
        "\n",
        "# Show SQL results\n",
        "print(\"Citations count (SQL):\")\n",
        "citations_sql_df.show(5, truncate=False)\n",
        "\n",
        "# Save SQL results as Parquet with gzip compression\n",
        "output_path_citations_sql = \"/tmp/data/citations_count_sql.parquet\"\n",
        "citations_sql_df.write.parquet(output_path_citations_sql, compression=\"gzip\", mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved Parquet files\n",
        "print(f\"Number of partitions (SQL): {citations_sql_df.rdd.getNumPartitions()}\")\n",
        "!ls -lh {output_path_citations_sql}\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data:\n",
            "+----------------+\n",
            "|value           |\n",
            "+----------------+\n",
            "|\"CITING\",\"CITED\"|\n",
            "|3858241,956203  |\n",
            "|3858241,1324234 |\n",
            "|3858241,3398406 |\n",
            "|3858241,3557384 |\n",
            "+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Split data:\n",
            "+-------+-------+\n",
            "|Citing |Cited  |\n",
            "+-------+-------+\n",
            "|3858241|956203 |\n",
            "|3858241|1324234|\n",
            "|3858241|3398406|\n",
            "|3858241|3557384|\n",
            "|3858241|3634889|\n",
            "+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Citations count (SQL):\n",
            "+---------+----------+\n",
            "|PatentNum|ncitations|\n",
            "+---------+----------+\n",
            "|1        |2         |\n",
            "|13       |2         |\n",
            "|24       |1         |\n",
            "|29       |1         |\n",
            "|31       |2         |\n",
            "+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Number of partitions (SQL): 2\n",
            "total 6.5M\n",
            "-rw-r--r-- 1 root root 3.2M Nov 29 14:20 part-00000-2fdbdece-3d1e-4d6d-81a5-ca8e257e18d0-c000.gz.parquet\n",
            "-rw-r--r-- 1 root root 3.3M Nov 29 14:20 part-00001-2fdbdece-3d1e-4d6d-81a5-ca8e257e18d0-c000.gz.parquet\n",
            "-rw-r--r-- 1 root root    0 Nov 29 14:20 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the apat63_99.txt file\n",
        "apat_path = os.environ[\"DRIVE_DATA\"] + \"apat63_99.txt\"\n",
        "\n",
        "# Specify the correct delimiter and header\n",
        "apat_df = spark.read.csv(apat_path, sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the raw DataFrame to confirm correct loading\n",
        "apat_df.show(5, truncate=False)\n",
        "\n",
        "# Select relevant columns and rename them\n",
        "apat_selected_df = (\n",
        "    apat_df.select(F.col(\"PATENT\").alias(\"PatentNum\"),\n",
        "                   F.col(\"COUNTRY\").alias(\"country\"),\n",
        "                   F.col(\"GYEAR\").alias(\"Year\"))\n",
        ")\n",
        "\n",
        "# Show the selected columns\n",
        "apat_selected_df.show(5, truncate=False)\n",
        "\n",
        "# Save as Parquet with gzip compression\n",
        "output_path_apat = \"/tmp/data/apat_selected.parquet\"\n",
        "apat_selected_df.write.parquet(output_path_apat, compression=\"gzip\", mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved Parquet files\n",
        "print(f\"Number of partitions: {apat_selected_df.rdd.getNumPartitions()}\")\n",
        "!ls -lh {output_path_apat}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qnA4a7aSkyK",
        "outputId": "f2a4216d-dfec-41e0-ceac-75b4df86545b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
            "|PATENT |GYEAR|GDATE|APPYEAR|COUNTRY|POSTATE|ASSIGNEE|ASSCODE|CLAIMS|NCLASS|CAT|SUBCAT|CMADE|CRECEIVE|RATIOCIT|GENERAL|ORIGINAL|FWDAPLAG|BCKGTLAG|SELFCTUB|SELFCTLB|SECDUPBD|SECDLWBD|\n",
            "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
            "|3070801|1963 |1096 |NULL   |BE     |NULL   |NULL    |1      |NULL  |269   |6  |69    |NULL |1       |NULL    |0.0    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "|3070802|1963 |1096 |NULL   |US     |TX     |NULL    |1      |NULL  |2     |6  |63    |NULL |0       |NULL    |NULL   |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "|3070803|1963 |1096 |NULL   |US     |IL     |NULL    |1      |NULL  |2     |6  |63    |NULL |9       |NULL    |0.3704 |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "|3070804|1963 |1096 |NULL   |US     |OH     |NULL    |1      |NULL  |2     |6  |63    |NULL |3       |NULL    |0.6667 |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "|3070805|1963 |1096 |NULL   |US     |CA     |NULL    |1      |NULL  |2     |6  |63    |NULL |1       |NULL    |0.0    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |NULL    |\n",
            "+-------+-----+-----+-------+-------+-------+--------+-------+------+------+---+------+-----+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+-------+----+\n",
            "|PatentNum|country|Year|\n",
            "+---------+-------+----+\n",
            "|3070801  |BE     |1963|\n",
            "|3070802  |US     |1963|\n",
            "|3070803  |US     |1963|\n",
            "|3070804  |US     |1963|\n",
            "|3070805  |US     |1963|\n",
            "+---------+-------+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Number of partitions: 2\n",
            "total 5.1M\n",
            "-rw-r--r-- 1 root root 2.8M Nov 29 14:25 part-00000-8fe5bb74-4feb-4813-b6a4-ce258a98c07f-c000.gz.parquet\n",
            "-rw-r--r-- 1 root root 2.4M Nov 29 14:25 part-00001-8fe5bb74-4feb-4813-b6a4-ce258a98c07f-c000.gz.parquet\n",
            "-rw-r--r-- 1 root root    0 Nov 29 14:25 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF1_LXZEnzMn"
      },
      "source": [
        "## Exercise 12.2\n",
        "\n",
        "Write a code that, from the Parquet files created in the previous exercise, obtains for each country and for each year: the total number of patents, the total number of citations from those patents, the average number of citations and the maximum number of citations. Compute only those values in which there are any values in both files (*inner join*). In addition, each country must show its whole name, obtained from the *country_codes.txt* file. The final DataFrame must look like this one:\n",
        "\n",
        "\n",
        "|Country            |Year|PatentsNum |TotalCitations|AvgCitations      |MaxCitations|\n",
        "|-------------------|----|-----------|--------------|------------------|------------|\n",
        "|Algeria            |1963|2          |7             |3.5               |4           |\n",
        "|Algeria            |1968|1          |2             |2.0               |2           |\n",
        "|Algeria            |1970|1          |2             |2.0               |2           |\n",
        "|Algeria            |1972|1          |1             |1.0               |1           |\n",
        "|Algeria            |1977|1          |2             |2.0               |2           |\n",
        "|Andorra            |1987|1          |3             |3.0               |3           |\n",
        "|Andorra            |1993|1          |1             |1.0               |1           |\n",
        "|Andorra            |1998|1          |1             |1.0               |1           |\n",
        "|Antigua and Barbuda|1978|1          |6             |6.0               |6           |\n",
        "|Antigua and Barbuda|1979|1          |14            |14.0              |14          |\n",
        "|Antigua and Barbuda|1991|1          |8             |8.0               |8           |\n",
        "|Antigua and Barbuda|1994|1          |19            |19.0              |19          |\n",
        "|Antigua and Barbuda|1995|2          |12            |6.0               |11          |\n",
        "|Antigua and Barbuda|1996|2          |3             |1.5               |2           |\n",
        "|Argentina          |1963|14         |35            |2.5               |7           |\n",
        "|Argentina          |1964|20         |60            |3.0               |8           |\n",
        "|Argentina          |1965|10         |35            |3.5               |10          |\n",
        "|Argentina          |1966|16         |44            |2.75              |9           |\n",
        "|Argentina          |1967|13         |60            |4.615384615384615 |14          |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The output DataFrame must be saved in a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the Parquet files\n",
        "citations_df = spark.read.parquet(\"/tmp/data/citations_count.parquet\")\n",
        "patents_df = spark.read.parquet(\"/tmp/data/apat_selected.parquet\")\n",
        "\n",
        "# Show the loaded data\n",
        "print(\"Citations DataFrame:\")\n",
        "citations_df.show(5, truncate=False)\n",
        "\n",
        "print(\"Patents DataFrame:\")\n",
        "patents_df.show(5, truncate=False)\n",
        "\n",
        "# Inner join between patents and citations using PatentNum\n",
        "joined_df = patents_df.join(\n",
        "    citations_df,\n",
        "    patents_df.PatentNum == citations_df.Cited,\n",
        "    how=\"inner\"\n",
        ").drop(\"Cited\")  # Drop redundant column\n",
        "\n",
        "# Group and aggregate\n",
        "aggregated_df = joined_df.groupBy(\"country\", \"Year\").agg(\n",
        "    F.count(\"PatentNum\").alias(\"PatentsNum\"),\n",
        "    F.sum(\"ncitations\").alias(\"TotalCitations\"),\n",
        "    F.avg(\"ncitations\").alias(\"AvgCitations\"),\n",
        "    F.max(\"ncitations\").alias(\"MaxCitations\")\n",
        ")\n",
        "\n",
        "# Load the country codes file with the correct separator (tab-separated)\n",
        "country_codes_path = os.environ[\"DRIVE_DATA\"] + \"country_codes.txt\"\n",
        "country_codes_df = spark.read.csv(country_codes_path, sep=\"\\t\", header=False, inferSchema=True)\n",
        "\n",
        "# Rename columns for clarity\n",
        "country_codes_df = country_codes_df.withColumnRenamed(\"_c0\", \"code\").withColumnRenamed(\"_c1\", \"name\")\n",
        "\n",
        "# Verify the schema of country_codes_df\n",
        "print(\"Country Codes DataFrame:\")\n",
        "country_codes_df.show(5, truncate=False)\n",
        "\n",
        "# Join to replace country codes with full names and sort by Country and Year\n",
        "final_df = aggregated_df.join(\n",
        "    country_codes_df,\n",
        "    aggregated_df.country == country_codes_df.code,\n",
        "    how=\"inner\"\n",
        ").select(\n",
        "    F.col(\"name\").alias(\"Country\"),\n",
        "    \"Year\",\n",
        "    \"PatentsNum\",\n",
        "    \"TotalCitations\",\n",
        "    \"AvgCitations\",\n",
        "    \"MaxCitations\"\n",
        ").orderBy(\"Country\", \"Year\")  # Sort by Country (alphabetical) and Year (ascending)\n",
        "\n",
        "# Show the final DataFrame\n",
        "print(\"Final DataFrame (sorted):\")\n",
        "final_df.show(10, truncate=False)\n",
        "\n",
        "# Save the final DataFrame as a single CSV file\n",
        "output_path = \"/tmp/data/final_patents_citations.csv\"\n",
        "final_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved CSV file\n",
        "print(\"Saved CSV File:\")\n",
        "!ls -lh {output_path}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxyt7LAWUTQ7",
        "outputId": "1ef4c10b-b8a8-4928-fd4d-74c333aa7d67"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citations DataFrame:\n",
            "+-------+----------+\n",
            "|Cited  |ncitations|\n",
            "+-------+----------+\n",
            "|3858272|3         |\n",
            "|3858273|3         |\n",
            "|3858276|6         |\n",
            "|3858277|1         |\n",
            "|3858278|3         |\n",
            "+-------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Patents DataFrame:\n",
            "+---------+-------+----+\n",
            "|PatentNum|country|Year|\n",
            "+---------+-------+----+\n",
            "|3070801  |BE     |1963|\n",
            "|3070802  |US     |1963|\n",
            "|3070803  |US     |1963|\n",
            "|3070804  |US     |1963|\n",
            "|3070805  |US     |1963|\n",
            "+---------+-------+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "Country Codes DataFrame:\n",
            "+----+--------------+\n",
            "|code|name          |\n",
            "+----+--------------+\n",
            "|AF  |Afghanistan   |\n",
            "|AX  |Aland Islands |\n",
            "|AL  |Albania       |\n",
            "|DZ  |Algeria       |\n",
            "|AS  |American Samoa|\n",
            "+----+--------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Final DataFrame (sorted):\n",
            "+-------------------+----+----------+--------------+------------+------------+\n",
            "|Country            |Year|PatentsNum|TotalCitations|AvgCitations|MaxCitations|\n",
            "+-------------------+----+----------+--------------+------------+------------+\n",
            "|Algeria            |1963|2         |7             |3.5         |4           |\n",
            "|Algeria            |1968|1         |2             |2.0         |2           |\n",
            "|Algeria            |1970|1         |2             |2.0         |2           |\n",
            "|Algeria            |1972|1         |1             |1.0         |1           |\n",
            "|Algeria            |1977|1         |2             |2.0         |2           |\n",
            "|Andorra            |1987|1         |3             |3.0         |3           |\n",
            "|Andorra            |1993|1         |1             |1.0         |1           |\n",
            "|Andorra            |1998|1         |1             |1.0         |1           |\n",
            "|Antigua and Barbuda|1978|1         |6             |6.0         |6           |\n",
            "|Antigua and Barbuda|1979|1         |14            |14.0        |14          |\n",
            "+-------------------+----+----------+--------------+------------+------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Saved CSV File:\n",
            "total 88K\n",
            "-rw-r--r-- 1 root root 86K Nov 29 14:35 part-00000-9de44225-4146-4aa0-ba12-3936a5cdd56c-c000.csv\n",
            "-rw-r--r-- 1 root root   0 Nov 29 14:35 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT3WU-1IwOPD"
      },
      "source": [
        "## Exercise 12.3\n",
        "\n",
        "From the apat63_99.txt file, obtain the number of patents per country and year **using RDDs** (do not use DataFrames). The resulting RDD must be a key/value RDD in which the key is a country and the value a list of tuples. Each tuple will be composed of a year and the number of patents of the country during that year. In addition, the resulting RDD must be sorted by  the country code and, for each country, values must be sorted by year.\n",
        "\n",
        "Example of output key/value entry:\n",
        "\n",
        "    (u'PA', [(u'1963', 2), (u'1964', 2), (u'1965', 1), (u'1966', 1), (u'1970', 1), (u'1971', 1), (u'1972', 6), (u'1974', 3), (u'1975', 5), (u'1976', 3), (u'1977', 2), (u'1978', 2), (u'1980', 2), (u'1982', 1), (u'1983', 1), (u'1985', 2), (u'1986', 1), (u'1987', 2), (u'1988', 1), (u'1990', 1), (u'1991', 2), (u'1993', 1), (u'1995', 1), (u'1996', 1), (u'1999', 1)])\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- You must remove the double quotation marks from the country code.\n",
        "- Use 8 partitions to read the apat63_99.txt.bz2 file.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apat_rdd = spark.sparkContext.textFile(os.environ[\"DRIVE_DATA\"] + \"apat63_99.txt\", minPartitions=8)\n",
        "\n",
        "# Remove the header row\n",
        "header = apat_rdd.first()\n",
        "print(f\"Header: {header}\")\n",
        "apat_rdd = apat_rdd.filter(lambda line: line != header)\n",
        "print(\"First 5 lines after removing header:\")\n",
        "print(apat_rdd.take(5))\n",
        "\n",
        "# Parse the data and handle malformed lines\n",
        "def parse_line(line):\n",
        "    fields = line.split(\",\")\n",
        "    if len(fields) > 4:  # Ensure there are enough fields\n",
        "        country = fields[4].replace('\"', '')  # Remove double quotes from the country code\n",
        "        year = fields[1]\n",
        "        return country, year\n",
        "    else:\n",
        "        return None  # Return None for malformed lines\n",
        "\n",
        "# Apply parsing and filter out invalid rows\n",
        "parsed_rdd = apat_rdd.map(parse_line)\n",
        "print(\"First 5 parsed lines (including None for malformed):\")\n",
        "print(parsed_rdd.take(5))\n",
        "\n",
        "parsed_rdd = parsed_rdd.filter(lambda x: x is not None)\n",
        "print(\"First 5 valid parsed lines:\")\n",
        "print(parsed_rdd.take(5))\n",
        "\n",
        "# Map to ((country, year), 1), reduce to count patents per (country, year)\n",
        "country_year_rdd = (\n",
        "    parsed_rdd.map(lambda pair: ((pair[0], pair[1]), 1))  # Map to ((country, year), 1)\n",
        "    .reduceByKey(lambda x, y: x + y)  # Count patents per (country, year)\n",
        ")\n",
        "\n",
        "print(\"First 5 entries after reduceByKey:\")\n",
        "print(country_year_rdd.take(5))  # Print reduced key-value pairs\n",
        "\n",
        "# Transform to (country, (year, count)) and group by country\n",
        "grouped_rdd = (\n",
        "    country_year_rdd.map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))  # Transform to (country, (year, count))\n",
        "    .groupByKey()  # Group by country\n",
        ")\n",
        "\n",
        "print(\"First 5 grouped entries by country:\")\n",
        "print(grouped_rdd.mapValues(list).take(5))  # Print grouped entries (convert iterator to list for printing)\n",
        "\n",
        "# Sort by year for each country and then by country code\n",
        "sorted_rdd = (\n",
        "    grouped_rdd.mapValues(lambda years: sorted(list(years), key=lambda x: x[0]))  # Sort by year for each country\n",
        "    .sortByKey()\n",
        ")\n",
        "\n",
        "result = sorted_rdd.collect()\n",
        "print(\"Final result (first 5 entries):\")\n",
        "print(result[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MVD0qfWWZmQ",
        "outputId": "33647801-9044-4dd7-8b51-ed514ce31103"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header: \"PATENT\",\"GYEAR\",\"GDATE\",\"APPYEAR\",\"COUNTRY\",\"POSTATE\",\"ASSIGNEE\",\"ASSCODE\",\"CLAIMS\",\"NCLASS\",\"CAT\",\"SUBCAT\",\"CMADE\",\"CRECEIVE\",\"RATIOCIT\",\"GENERAL\",\"ORIGINAL\",\"FWDAPLAG\",\"BCKGTLAG\",\"SELFCTUB\",\"SELFCTLB\",\"SECDUPBD\",\"SECDLWBD\"\n",
            "First 5 lines after removing header:\n",
            "['3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,', '3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,', '3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,', '3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,', '3070805,1963,1096,,\"US\",\"CA\",,1,,2,6,63,,1,,0,,,,,,,']\n",
            "First 5 parsed lines (including None for malformed):\n",
            "[('BE', '1963'), ('US', '1963'), ('US', '1963'), ('US', '1963'), ('US', '1963')]\n",
            "First 5 valid parsed lines:\n",
            "[('BE', '1963'), ('US', '1963'), ('US', '1963'), ('US', '1963'), ('US', '1963')]\n",
            "First 5 entries after reduceByKey:\n",
            "[(('AT', '1963'), 86), (('SE', '1963'), 386), (('MA', '1963'), 3), (('VE', '1963'), 9), (('CU', '1963'), 8)]\n",
            "First 5 grouped entries by country:\n",
            "[('CH', [('1977', 1347), ('1979', 1025), ('1980', 1266), ('1981', 1239), ('1994', 1169), ('1972', 1305), ('1974', 1454), ('1984', 1174), ('1996', 1112), ('1997', 1090), ('1969', 1058), ('1970', 1112), ('1978', 1330), ('1983', 1016), ('1993', 1127), ('1963', 668), ('1966', 983), ('1967', 948), ('1971', 1281), ('1973', 1326), ('1988', 1245), ('1998', 1278), ('1964', 666), ('1982', 1147), ('1990', 1284), ('1995', 1056), ('1968', 822), ('1985', 1233), ('1986', 1211), ('1999', 1279), ('1991', 1335), ('1965', 862), ('1975', 1456), ('1976', 1475), ('1987', 1374), ('1989', 1363), ('1992', 1197)]), ('NO', [('1977', 106), ('1979', 80), ('1980', 79), ('1981', 93), ('1994', 126), ('1972', 88), ('1974', 91), ('1984', 87), ('1996', 139), ('1997', 142), ('1969', 69), ('1970', 68), ('1978', 89), ('1983', 66), ('1993', 117), ('1963', 33), ('1966', 59), ('1967', 49), ('1971', 77), ('1973', 84), ('1988', 121), ('1998', 198), ('1964', 43), ('1982', 65), ('1990', 112), ('1995', 130), ('1968', 49), ('1985', 90), ('1986', 81), ('1999', 224), ('1991', 111), ('1965', 52), ('1975', 103), ('1976', 103), ('1987', 135), ('1989', 126), ('1992', 108)]), ('BB', [('1977', 2), ('1967', 1), ('1988', 1), ('1986', 1), ('1999', 1), ('1975', 2), ('1989', 1)]), ('LI', [('1977', 11), ('1979', 8), ('1980', 18), ('1981', 20), ('1994', 17), ('1972', 10), ('1974', 15), ('1984', 16), ('1996', 12), ('1997', 11), ('1969', 14), ('1970', 23), ('1978', 10), ('1983', 12), ('1993', 11), ('1963', 5), ('1966', 12), ('1967', 13), ('1971', 28), ('1973', 14), ('1988', 10), ('1998', 16), ('1964', 10), ('1982', 19), ('1990', 15), ('1995', 17), ('1968', 15), ('1985', 13), ('1986', 17), ('1999', 15), ('1991', 11), ('1965', 14), ('1975', 13), ('1976', 21), ('1987', 16), ('1989', 11), ('1992', 16)]), ('NG', [('1977', 1), ('1980', 2), ('1981', 2), ('1994', 1), ('1974', 1), ('1997', 2), ('1969', 1), ('1970', 1), ('1978', 3), ('1967', 2), ('1971', 4), ('1998', 2), ('1968', 2), ('1985', 1), ('1991', 1), ('1975', 1), ('1976', 1), ('1992', 2)])]\n",
            "Final result (first 5 entries):\n",
            "[('AD', [('1987', 1), ('1993', 1), ('1995', 1), ('1998', 2), ('1999', 1)]), ('AE', [('1984', 2), ('1985', 2), ('1987', 1), ('1989', 2), ('1990', 1), ('1991', 2), ('1992', 1), ('1993', 1), ('1994', 1), ('1996', 1), ('1998', 1), ('1999', 3)]), ('AG', [('1978', 1), ('1979', 1), ('1991', 1), ('1994', 1), ('1995', 2), ('1996', 2), ('1999', 1)]), ('AI', [('1998', 1)]), ('AL', [('1999', 1)])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0LiWoc4VQdh"
      },
      "source": [
        "## Exercise 12.4\n",
        "\n",
        "From the Parquet files created in Exercise 12.1, create a DataFrame that gives the patent or patents with the higher number of citations per country and year, as well as the average of the number of citations per country and year, and the difference between the maximum and the average values. The resulting DataFrame should look like this:\n",
        "\n",
        "\n",
        "|Country|Year|PatentNum|max  |average       |diff              |\n",
        "|-------|----|---------|-----|--------------|------------------|\n",
        "|AD     |1987|4688621  |3    |3.0           |0.0               |\n",
        "|AD     |1993|5193231  |1    |1.0           |0.0               |\n",
        "|AD     |1998|5765303  |1    |1.0           |0.0               |\n",
        "|AE     |1984|4482959  |5    |5.0           |0.0               |\n",
        "|AE     |1985|4554981  |14   |14.0          |0.0               |\n",
        "|AE     |1987|4663181  |3    |3.0           |0.0               |\n",
        "|AE     |1989|4805221  |7    |5.0           |2.0               |\n",
        "|AE     |1990|4909321  |2    |2.0           |0.0               |\n",
        "|AE     |1991|5004552  |3    |2.0           |1.0               |\n",
        "|AE     |1992|5104556  |4    |4.0           |0.0               |\n",
        "|AE     |1993|5181569  |8    |8.0           |0.0               |\n",
        "|AE     |1996|5580125  |1    |1.0           |0.0               |\n",
        "|AG     |1978|4126850  |6    |6.0           |0.0               |\n",
        "|AG     |1979|4172981  |14   |14.0          |0.0               |\n",
        "|AG     |1991|5013035  |8    |8.0           |0.0               |\n",
        "|AG     |1994|5345071  |19   |19.0          |0.0               |\n",
        "|AG     |1995|5457307  |11   |6.0           |5.0               |\n",
        "|AG     |1996|5525786  |2    |1.5           |0.5               |\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Parquet files created in Exercise 12.1\n",
        "citations_df = spark.read.parquet(\"/tmp/data/citations_count.parquet\")\n",
        "patents_df = spark.read.parquet(\"/tmp/data/apat_selected.parquet\")\n",
        "\n",
        "# Create temporary views for the DataFrames\n",
        "citations_df.createOrReplaceTempView(\"citations\")\n",
        "patents_df.createOrReplaceTempView(\"patents\")\n",
        "\n",
        "# SQL Query 1: Join patents and citations\n",
        "joined_query = \"\"\"\n",
        "    SELECT\n",
        "        p.country,\n",
        "        p.Year,\n",
        "        p.PatentNum,\n",
        "        c.ncitations\n",
        "    FROM patents p\n",
        "    INNER JOIN citations c\n",
        "    ON p.PatentNum = c.Cited\n",
        "\"\"\"\n",
        "joined_df = spark.sql(joined_query)\n",
        "joined_df.createOrReplaceTempView(\"joined\")\n",
        "\n",
        "# SQL Query 2: Compute max and average citations\n",
        "aggregated_query = \"\"\"\n",
        "    SELECT\n",
        "        country,\n",
        "        Year,\n",
        "        MAX(ncitations) AS max,\n",
        "        AVG(ncitations) AS average\n",
        "    FROM joined\n",
        "    GROUP BY country, Year\n",
        "\"\"\"\n",
        "aggregated_df = spark.sql(aggregated_query)\n",
        "aggregated_df.createOrReplaceTempView(\"aggregated\")\n",
        "\n",
        "# SQL Query 3: Find patents with the maximum number of citations\n",
        "result_query = \"\"\"\n",
        "    SELECT DISTINCT\n",
        "        agg.country AS Country,\n",
        "        agg.Year AS Year,\n",
        "        j.PatentNum AS PatentNum,\n",
        "        agg.max AS max,\n",
        "        agg.average AS average,\n",
        "        (agg.max - agg.average) AS diff\n",
        "    FROM aggregated agg\n",
        "    INNER JOIN joined j\n",
        "    ON agg.country = j.country\n",
        "       AND agg.Year = j.Year\n",
        "       AND agg.max = j.ncitations\n",
        "    ORDER BY Country, Year\n",
        "\"\"\"\n",
        "result_df = spark.sql(result_query)\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "print(\"Final DataFrame:\")\n",
        "result_df.show(10, truncate=False)\n",
        "\n",
        "# Save the resulting DataFrame as a single CSV file with a header and no compression\n",
        "output_path = \"/tmp/data/patents_max_avg_diff_sql.csv\"\n",
        "result_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved CSV file\n",
        "print(\"Saved CSV File:\")\n",
        "!ls -lh {output_path}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt8r20goZ6sX",
        "outputId": "1cd63337-a8e9-44b3-ec41-6c89b717528a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final DataFrame:\n",
            "+-------+----+---------+---+-------+----+\n",
            "|Country|Year|PatentNum|max|average|diff|\n",
            "+-------+----+---------+---+-------+----+\n",
            "|AD     |1987|4688621  |3  |3.0    |0.0 |\n",
            "|AD     |1993|5193231  |1  |1.0    |0.0 |\n",
            "|AD     |1998|5765303  |1  |1.0    |0.0 |\n",
            "|AE     |1984|4482959  |5  |5.0    |0.0 |\n",
            "|AE     |1985|4554981  |14 |14.0   |0.0 |\n",
            "|AE     |1987|4663181  |3  |3.0    |0.0 |\n",
            "|AE     |1989|4805221  |7  |5.0    |2.0 |\n",
            "|AE     |1990|4909321  |2  |2.0    |0.0 |\n",
            "|AE     |1991|5004552  |3  |2.0    |1.0 |\n",
            "|AE     |1992|5104556  |4  |4.0    |0.0 |\n",
            "+-------+----+---------+---+-------+----+\n",
            "only showing top 10 rows\n",
            "\n",
            "Saved CSV File:\n",
            "total 116K\n",
            "-rw-r--r-- 1 root root 115K Nov 29 14:55 part-00000-42bfd561-6a31-4582-859e-e6fd467039a6-c000.csv\n",
            "-rw-r--r-- 1 root root    0 Nov 29 14:55 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektPyLWzaImT"
      },
      "source": [
        "## Exercise 12.5\n",
        "\n",
        "From the Parquet file with the (PatentNum,Country,Year) information from Exercise 12.1, create a DataFrame that shows the number of patents associated to each country per decade (understanding as a *decade* the years from 0 to 9; e.g. from 1970 to 1979). In addition, the DataFrame must show the increase or decrease of the number of patents per country and decade with respect to the previous decade. The resulting DataFrame must look like this:\n",
        "\n",
        "|Country|Decade|PatentsNum|Diff|\n",
        "|-------|------|----------|----|\n",
        "|AD     |1980  |1         |0   |\n",
        "|AD     |1990  |5         |4   |\n",
        "|AE     |1980  |7         |0   |\n",
        "|AE     |1990  |11        |4   |\n",
        "|AG     |1970  |2         |0   |\n",
        "|AG     |1990  |7         |5   |\n",
        "|AI     |1990  |1         |0   |\n",
        "|AL     |1990  |1         |0   |\n",
        "|AM     |1990  |2         |0   |\n",
        "|AN     |1970  |1         |0   |\n",
        "|AN     |1980  |2         |1   |\n",
        "|AN     |1990  |5         |3   |\n",
        "|AR     |1960  |135       |0   |\n",
        "|AR     |1970  |239       |104 |\n",
        "|AR     |1980  |184       |-55 |\n",
        "|AR     |1990  |292       |108 |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the Parquet file with PatentNum, Country, and Year information\n",
        "patents_df = spark.read.parquet(\"/tmp/data/apat_selected.parquet\")\n",
        "\n",
        "# Compute the decade for each year\n",
        "patents_with_decade_df = patents_df.withColumn(\n",
        "    \"Decade\",\n",
        "    (F.col(\"Year\") / 10).cast(\"int\") * 10  # Compute the decade by truncating the year to its decade\n",
        ")\n",
        "\n",
        "# Group by country and decade, and count the number of patents\n",
        "grouped_df = patents_with_decade_df.groupBy(\"country\", \"Decade\").agg(\n",
        "    F.count(\"PatentNum\").alias(\"PatentsNum\")  # Count the number of patents\n",
        ")\n",
        "\n",
        "# Use a window function to compute the difference with the previous decade\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define a window partitioned by country and ordered by decade\n",
        "window_spec = Window.partitionBy(\"country\").orderBy(\"Decade\")\n",
        "\n",
        "# Compute the difference with the previous decade\n",
        "result_df = grouped_df.withColumn(\n",
        "    \"Diff\",\n",
        "    F.col(\"PatentsNum\") - F.lag(\"PatentsNum\", 1).over(window_spec)  # Compute difference with the previous decade\n",
        ").fillna(0, subset=[\"Diff\"])  # Fill null differences (first decade) with 0\n",
        "\n",
        "final_df = result_df.orderBy(\"country\", \"Decade\")\n",
        "\n",
        "print(\"Final DataFrame:\")\n",
        "final_df.show(10, truncate=False)\n",
        "\n",
        "# Save the resulting DataFrame as a single CSV file with a header and no compression\n",
        "output_path = \"/tmp/data/patents_per_decade.csv\"\n",
        "final_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "print(\"Saved CSV File:\")\n",
        "!ls -lh {output_path}\n"
      ],
      "metadata": {
        "id": "MsnXojUSeRyr",
        "outputId": "cefa2058-47a4-459a-e113-98473b62dcde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final DataFrame:\n",
            "+-------+------+----------+----+\n",
            "|country|Decade|PatentsNum|Diff|\n",
            "+-------+------+----------+----+\n",
            "|AD     |1980  |1         |0   |\n",
            "|AD     |1990  |5         |4   |\n",
            "|AE     |1980  |7         |0   |\n",
            "|AE     |1990  |11        |4   |\n",
            "|AG     |1970  |2         |0   |\n",
            "|AG     |1990  |7         |5   |\n",
            "|AI     |1990  |1         |0   |\n",
            "|AL     |1990  |1         |0   |\n",
            "|AM     |1990  |2         |0   |\n",
            "|AN     |1970  |1         |0   |\n",
            "+-------+------+----------+----+\n",
            "only showing top 10 rows\n",
            "\n",
            "Saved CSV File:\n",
            "total 8.0K\n",
            "-rw-r--r-- 1 root root 6.3K Nov 29 15:10 part-00000-a64a3c66-3fd4-42f4-9e60-e2ebab8f708d-c000.csv\n",
            "-rw-r--r-- 1 root root    0 Nov 29 15:10 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WpsudQWWeSPe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}