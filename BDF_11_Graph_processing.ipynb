{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulSerin/Big-Data-Framework/blob/main/BDF_11_Graph_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf",
        "outputId": "c937a4c6-53cf-42d9-daf3-4d7dfdec29e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.2.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  https://archive.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [1 InRelease 3,626 B/3,\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [Connected to r2u.stat.\r                                                                                                    \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,172 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,619 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,224 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,506 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,452 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,512 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,736 kB]\n",
            "Fetched 20.6 MB in 4s (4,760 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2024-11-26 15:16:48--  https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop2.7.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 272866820 (260M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.2.3-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-3.2.3-bin-had 100%[===================>] 260.23M   805KB/s    in 3m 23s  \n",
            "\n",
            "2024-11-26 15:20:12 (1.28 MB/s) - ‘spark-3.2.3-bin-hadoop2.7.tgz’ saved [272866820/272866820]\n",
            "\n",
            "spark-3.2.3-bin-hadoop2.7.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W8ScIqhafBT",
        "outputId": "bad8505f-3d3e-453b-c426-860576090c91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-26 15:20:20--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 3.168.132.68, 3.168.132.8, 3.168.132.114, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|3.168.132.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247880 (242K) [binary/octet-stream]\n",
            "Saving to: ‘graphframes-0.8.2-spark3.2-s_2.12.jar’\n",
            "\n",
            "\r          graphfram   0%[                    ]       0  --.-KB/s               \rgraphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-11-26 15:20:21 (8.12 MB/s) - ‘graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx",
        "outputId": "0bd90fe8-f85d-437f-b825-104237e90817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Big Data Framework/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop2.7 /content/spark\n",
        "\n",
        "!mv graphframes-0.8.2-spark3.2-s_2.12.jar /content/spark/jars/\n",
        "\n",
        "!export SPARK_HOME=/content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\"\n",
        "\n",
        "!ls -l /content/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/spark': No such file or directory\n",
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/Big Data Framework/data/\n",
            "total 266484\n",
            "drwxr-xr-x  1 root root      4096 Nov 22 14:23 sample_data\n",
            "lrwxrwxrwx  1 root root        34 Nov 26 15:20 spark -> /content/spark-3.2.3-bin-hadoop2.7\n",
            "drwxr-xr-x 13  501 1000      4096 Nov 14  2022 spark-3.2.3-bin-hadoop2.7\n",
            "-rw-r--r--  1 root root 272866820 Nov 14  2022 spark-3.2.3-bin-hadoop2.7.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN",
        "outputId": "0fc4b1a9-f984-45e8-dd0a-24ba0edf2f4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "sc.addPyFile('/content/spark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar')\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "PySpark version 3.2.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"packages\",\"graphframes:graphframes-0.8.2-spark3.2-s_2.12\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT",
        "outputId": "479451c6-7323-475b-b5ee-daf12401e3d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 11 - Graph processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc96GG2FFzA5"
      },
      "source": [
        "## GraphX: Graph processing with RDDs\n",
        "\n",
        "Parallel graph programming using Spark\n",
        "\n",
        "- Main abstraction: [*Graph*](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.Graph)\n",
        "    -   Directed multigraph with properties assigned to vertices and edges\n",
        "    -   It is an extension of the RDDs\n",
        "- It includes graph constructors, basic operators ( *reverse*, *subgraph*…) and graph algorithms ( *PageRank*, *Triangle Counting*…)\n",
        "- Only availabe on Scala.\n",
        "\n",
        "Documentation: [spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)\n",
        "\n",
        "API: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szq16u_cIKxA"
      },
      "source": [
        "## Graphs in GraphX\n",
        "<img src=\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/grapxgraph.png\" alt=\"Grafo en GraphX\" style=\"width: 50px;\"/>\n",
        "(Source: M.S. Malak, R. East \"Spark GraphX in action\", Manning, 2016)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HkF-qEfIZrO"
      },
      "source": [
        "### Example of a simple graph\n",
        "<img src=\"http://persoal.citius.usc.es/tf.pena/TCDM/figs/simpsonsgraph.png\" alt=\"Grafo de los Simpson\" style=\"width: 600px;\"/>\n",
        "(Source: P. Zecević, M. Bonaći \"Spark in action\", Manning, 2017)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6bmE-QvGf2r"
      },
      "source": [
        "## GraphFrames: : Graph processing with DataFrames\n",
        "\n",
        "In Python we can use [*GraphFrames*](https://graphframes.github.io/graphframes/docs/_site/quick-start.html) which wraps GraphX algorithms under the DataFrames API, providing a Python interface.\n",
        "\n",
        "- Support for multiple languages is on the works\n",
        "    - For now,  available for Scala and Python\n",
        "- Not yet integrated on Spark\n",
        "    - Available as an external package (https://spark-packages.org/package/graphframes/graphframes)\n",
        "\n",
        "More information:\n",
        "- Project web: https://graphframes.github.io/graphframes/docs/_site/\n",
        "- Python API : https://graphframes.github.io/graphframes/docs/_site/api/python/index.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l07_p2RRIu_f"
      },
      "source": [
        "### Graphs using pyspark and GraphFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1enKNqmMIy1Z",
        "outputId": "ca2a866e-fcc8-481a-adb0-d66f8a11c80f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The following example shows how to create a GraphFrame, query it, and run the PageRank algorithm.\n",
        "# Source: https://graphframes.github.io/graphframes/docs/_site/quick-start.html\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "from graphframes import *\n",
        "\n",
        "# Create a Vertex DataFrame with unique ID column \"id\"\n",
        "v = sqlContext.createDataFrame([\n",
        "  (\"a\", \"Alice\", 34),\n",
        "  (\"b\", \"Bob\", 36),\n",
        "  (\"c\", \"Charlie\", 30),\n",
        "], [\"id\", \"name\", \"age\"])\n",
        "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
        "e = sqlContext.createDataFrame([\n",
        "  (\"a\", \"b\", \"friend\"),\n",
        "  (\"b\", \"c\", \"follow\"),\n",
        "  (\"c\", \"b\", \"follow\"),\n",
        "], [\"src\", \"dst\", \"relationship\"])\n",
        "# Create a GraphFrame\n",
        "\n",
        "g = GraphFrame(v, e)\n",
        "\n",
        "# Query: Get in-degree of each vertex.\n",
        "g.inDegrees.show()\n",
        "\n",
        "# Query: Count the number of \"follow\" connections in the graph.\n",
        "g.edges.filter(\"relationship = 'follow'\").count()\n",
        "\n",
        "# Run PageRank algorithm, and show results.\n",
        "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
        "results.vertices.select(\"id\", \"pagerank\").show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark/python/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+\n",
            "| id|inDegree|\n",
            "+---+--------+\n",
            "|  b|       2|\n",
            "|  c|       1|\n",
            "+---+--------+\n",
            "\n",
            "+---+------------------+\n",
            "| id|          pagerank|\n",
            "+---+------------------+\n",
            "|  c|1.8994109890559092|\n",
            "|  b|1.0905890109440908|\n",
            "|  a|              0.01|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIZQECRHuiDw"
      },
      "source": [
        "#Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usL5LynSI6rE"
      },
      "source": [
        "## Exercise 11.1:\n",
        "\n",
        "A long time ago in a galaxy far, far away, the characters of the Star Wars franchise interacted with each other in an endless series of films. An ancient Jedi order, called the *Data Guardians of the Galaxy* (not affiliated to Marvel's homonym :) registered all those interactions and saved them on a digital file so that they could be studied by the forthcoming generations. This file was originally called (guess it) `sw.txt`, and you will find it in the `/data` directory.\n",
        "\n",
        "Using pySpark, perform the following operations and answer the following questions:\n",
        "\n",
        "1. Load the `$DRIVE_DATA/sw.txt` file. Take into account that it is a JSON file.\n",
        "2. Using this information, create a graph of interactions between the Star Wars characters.\n",
        "3. How many different characters are there?\n",
        "4. How many interactions are there?\n",
        "5. Who is the central character in Star Wars (the one who interacts in most scenes)?\n",
        "6. Who is the character with the highest 'rank' in Star Wars (use the PageRank algorithm)?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import MULTILINE\n",
        "sw_DF = spark.read.json(os.environ[\"DRIVE_DATA\"] + \"sw.txt\").cache()\n",
        "sw_DF.printSchema()"
      ],
      "metadata": {
        "id": "FErn4CUw_dvR",
        "outputId": "f21d68d1-41f0-4fe0-978b-edb930b1b9e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2_LatkN_4Gn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}