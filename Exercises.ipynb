{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHb5sC5q0T8gQ5WfgxdT5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulSerin/Big-Data-Framework/blob/main/Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercises**"
      ],
      "metadata": {
        "id": "rF2yIjKnS_5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Spark environment setup*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HVKsdEbEVi3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtjPnwPPS_PC",
        "outputId": "34c2e9a8-5344-444b-96b7-f9a191435715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r-project\r                                                                                                    \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [1 InRelease 73.5 kB/129 kB 57%] [Connected t\r                                                                                                    \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 2s (166 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2024-11-29 15:42:03--  http://apache.osuosl.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 64.50.236.52, 140.211.166.134, 64.50.233.100, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|64.50.236.52|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400864419 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.3-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.3-bin-had 100%[===================>] 382.29M  74.2MB/s    in 7.9s    \n",
            "\n",
            "2024-11-29 15:42:11 (48.4 MB/s) - ‘spark-3.5.3-bin-hadoop3.tgz’ saved [400864419/400864419]\n",
            "\n",
            "spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Big Data Framework/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8maMDLUJThlo",
        "outputId": "6a6ea704-a099-475c-d0ac-78777f993425"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/Big Data Framework/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlieYwSpTjOX",
        "outputId": "83202981-919e-4761-cde6-3332be56860c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "PySpark version 3.5.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "EB18ZsqVTq_G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Wof7pCTtEm",
        "outputId": "d09020df-0973-4963-9bd0-8bcaad53fb68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ULPx4Y1LiR"
      },
      "source": [
        "## **Exercise 3.1: Word count**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of words per line in the $DRIVE_DATA/quijote.txt file.\n",
        "\n",
        "Repeat the exercise but this time counting the number of words in the whole file."
      ],
      "metadata": {
        "id": "sPAq-ILTkTgV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c7Q_ljrX5RtE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "# so that we can use the F.split() function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Méthode 1 (Dataframe)"
      ],
      "metadata": {
        "id": "fEjuV31kaqpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Charger le fichier texte dans un DataFrame\n",
        "file_path = os.environ[\"DRIVE_DATA\"] + \"quijote.txt\"\n",
        "df = spark.read.text(file_path).withColumnRenamed(\"value\", \"line\")\n",
        "\n",
        "# Étape 2 : Compter les mots par ligne\n",
        "df_with_word_count = df.withColumn(\"word_count\", F.size(F.split(F.col(\"line\"), \" \")))\n",
        "df_with_word_count.show(10, truncate=False)\n",
        "\n",
        "# Étape 3 : Compter les mots dans tout le fichier\n",
        "total_word_count = df_with_word_count.agg(F.sum(\"word_count\").alias(\"total_word_count\"))\n",
        "total_word_count.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrHIf1flWkM2",
        "outputId": "69c1d202-0086-4348-fd98-0578bdae11c6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------+----------+\n",
            "|line                                                                       |word_count|\n",
            "+---------------------------------------------------------------------------+----------+\n",
            "|The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra|12        |\n",
            "|                                                                           |1         |\n",
            "|This eBook is for the use of anyone anywhere at no cost and with           |14        |\n",
            "|almost no restrictions whatsoever.  You may copy it, give it away or       |13        |\n",
            "|re-use it under the terms of the Project Gutenberg License included        |11        |\n",
            "|with this eBook or online at www.gutenberg.net                             |7         |\n",
            "|                                                                           |1         |\n",
            "|                                                                           |1         |\n",
            "|Title: Don Quijote                                                         |3         |\n",
            "|                                                                           |1         |\n",
            "+---------------------------------------------------------------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------------+\n",
            "|total_word_count|\n",
            "+----------------+\n",
            "|          393764|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : this method counts 1 word for empty lines.\n",
        "\n",
        "We will filter thoses empty lines before"
      ],
      "metadata": {
        "id": "HW3Se-lWbyvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Méthode 2 (RDD)"
      ],
      "metadata": {
        "id": "_rvzdn8_amiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text file as an RDD\n",
        "rdd = sc.textFile(file_path)\n",
        "\n",
        "# Step 0: Filter out empty lines\n",
        "# Strip removes leading and trailing whitespaces, and we ensure the line is not empty\n",
        "rdd_non_empty = rdd.filter(lambda line: line.strip() != \"\")\n",
        "\n",
        "# Step 1: Count the number of words per line (excluding empty lines)\n",
        "# Split each non-empty line into words and count the number of words\n",
        "word_count_per_line = rdd_non_empty.map(lambda line: len(line.split(\" \")))\n",
        "print(\"Word count per line (excluding empty lines):\")\n",
        "print(word_count_per_line.take(10))  # Display the first 10 results\n",
        "\n",
        "# Step 2: Count the total number of words in the file (excluding empty lines)\n",
        "# Split all non-empty lines into words, flatten the resulting lists, and count the total\n",
        "total_word_count_rdd = rdd_non_empty.flatMap(lambda line: line.split(\" \")).count()\n",
        "print(\"Total word count in the file (excluding empty lines):\", total_word_count_rdd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph_2Rl7LX7o4",
        "outputId": "16386316-2588-4323-cca0-6b9fe3bcaf5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count per line (excluding empty lines):\n",
            "[12, 14, 13, 11, 7, 3, 5, 7, 4, 2]\n",
            "Total word count in the file (excluding empty lines): 387834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Méthode 3 (function SQL)"
      ],
      "metadata": {
        "id": "44Mg-Kd3auUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary view\n",
        "df.createOrReplaceTempView(\"lines\")\n",
        "\n",
        "# Step 1: Count the number of words per line (excluding empty lines)\n",
        "# Filter out rows where the line is NULL or empty\n",
        "word_count_query = \"\"\"\n",
        "SELECT line,\n",
        "       SIZE(SPLIT(line, ' ')) AS word_count\n",
        "FROM lines\n",
        "WHERE line IS NOT NULL AND line != ''\n",
        "\"\"\"\n",
        "df_word_count = spark.sql(word_count_query)\n",
        "df_word_count.show(10)  # Display the first 10 results\n",
        "\n",
        "# Step 2: Count the total number of words in the file (excluding empty lines)\n",
        "# Again, exclude rows where the line is NULL or empty\n",
        "total_word_count_query = \"\"\"\n",
        "SELECT SUM(SIZE(SPLIT(line, ' '))) AS total_word_count\n",
        "FROM lines\n",
        "WHERE line IS NOT NULL AND line != ''\n",
        "\"\"\"\n",
        "df_total_word_count = spark.sql(total_word_count_query)\n",
        "df_total_word_count.show()  # Display the total word count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ju6PfeAavez",
        "outputId": "0468e72e-5ee8-47f6-fef4-7e3602490150"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|                line|word_count|\n",
            "+--------------------+----------+\n",
            "|The Project Guten...|        12|\n",
            "|This eBook is for...|        14|\n",
            "|almost no restric...|        13|\n",
            "|re-use it under t...|        11|\n",
            "|with this eBook o...|         7|\n",
            "|  Title: Don Quijote|         3|\n",
            "|Author: Miguel de...|         5|\n",
            "|Posting Date: Apr...|         7|\n",
            "|Release Date: Dec...|         4|\n",
            "|   Language: Spanish|         2|\n",
            "+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------------+\n",
            "|total_word_count|\n",
            "+----------------+\n",
            "|          387834|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 4.1: Pi Estimation**"
      ],
      "metadata": {
        "id": "1YH9-PfYT1Xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Monte Carlo method, estimate the value of Pi. Use the random() method from the random class."
      ],
      "metadata": {
        "id": "Y410Vk6LT_-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YdVciAgNTu40"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With RDD :"
      ],
      "metadata": {
        "id": "V_MnlIAx3Epc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Pi Estimation\").getOrCreate()\n",
        "\n",
        "# Step 2: Number of points to simulate\n",
        "NUM_POINTS = 1000000\n",
        "\n",
        "# Step 3: Create an RDD with NUM_POINTS random samples\n",
        "rdd = spark.sparkContext.parallelize(range(NUM_POINTS))\n",
        "\n",
        "# Step 4: Function to determine if a point is inside the unit circle\n",
        "def is_inside_unit_circle(_):\n",
        "    x = random.random()\n",
        "    y = random.random()\n",
        "    return x**2 + y**2 <= 1\n",
        "\n",
        "# Step 5: Map points to 1 if inside circle, 0 otherwise\n",
        "points_inside_circle = rdd.map(is_inside_unit_circle).filter(lambda inside: inside == True).count()\n",
        "\n",
        "# Step 6: Calculate Pi\n",
        "pi_estimate = 4 * (points_inside_circle / NUM_POINTS)\n",
        "\n",
        "# Step 7: Output the result\n",
        "print(f\"Estimated value of Pi using {NUM_POINTS} points: {pi_estimate}\")\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5TMjlA5zDL4",
        "outputId": "7a2ef037-d77f-4e31-b134-eeaba8d1fc38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated value of Pi using 1000000 points: 3.141096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With a Dataframe :"
      ],
      "metadata": {
        "id": "3bUa_BvR2_a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand, pow, col\n",
        "\n",
        "# Step 0: Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Pi Estimation\").getOrCreate()\n",
        "\n",
        "# Step 1: Number of points to simulate\n",
        "NUM_POINTS = 1000000  # Increase for more accuracy\n",
        "\n",
        "# Step 2: Create a DataFrame with random x and y points\n",
        "df_points = (\n",
        "    spark.range(NUM_POINTS)\n",
        "    .select(\n",
        "        rand(seed=0).alias(\"x\"),\n",
        "        rand(seed=1).alias(\"y\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 3: Add a column to determine if the point is inside the unit circle\n",
        "df_inside_circle = df_points.withColumn(\n",
        "    \"inside_circle\", (pow(col(\"x\"), 2) + pow(col(\"y\"), 2) <= 1).cast(\"int\")\n",
        ")\n",
        "\n",
        "# Step 4: Count points inside the circle\n",
        "points_inside_circle = df_inside_circle.agg({\"inside_circle\": \"sum\"}).collect()[0][0]\n",
        "\n",
        "# Step 5: Calculate Pi\n",
        "pi_estimate = 4 * (points_inside_circle / NUM_POINTS)\n",
        "\n",
        "# Step 6: Output the result\n",
        "print(f\"Estimated value of Pi using {NUM_POINTS} points: {pi_estimate}\")\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nnpiWGIz_j8",
        "outputId": "e8f00144-6870-4702-dbe2-00ef9effdf8a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated value of Pi using 1000000 points: 3.137908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vPYCnS8ERmg"
      },
      "source": [
        "## Exercise 4.2: Inspect a log file\n",
        "\n",
        "Upload the file /var/log/syslog from your computer to this notebook. Then, select only the \"bad lines\": WARNING and ERROR messages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "RTo7oJej43Zv",
        "outputId": "ca13b97e-428b-41f6-e19f-0703be345076"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-24b3d2a1-e71f-4d8b-a85c-f8ea0196e365\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-24b3d2a1-e71f-4d8b-a85c-f8ea0196e365\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving syslog to syslog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Demr8raN4yIx",
        "outputId": "4d38b45b-f362-436b-f364-49eadbad66ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive\tsample_data  spark  spark-3.5.3-bin-hadoop3  syslog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VR-Men-G5ZdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3927f1-41a4-4a26-b1f4-80eb70aeb214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad lines (WARNING and ERROR messages):\n",
            "Nov 25 17:22:07 paul-serin ovpn-cytech.students[1358]: WARNING: file '/data/CYTECH-VPN-CLE-PERSONNELLE/client.p12' is group or others accessible\n",
            "Nov 25 17:22:07 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:22:07 paul-serin gnome-session[1456]: gnome-session-binary[1456]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 25 17:22:07 paul-serin gnome-session-binary[1456]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 25 17:22:15 paul-serin gnome-shell[1481]: JS WARNING: [resource:///org/gnome/shell/ui/layout.js 24]: reference to undefined property \"MetaWindowXwayland\"\n",
            "Nov 25 17:22:21 paul-serin gnome-session[2098]: gnome-session-binary[2098]: WARNING: Could not parse desktop file nautilus-autostart.desktop or it references a not found TryExec binary\n",
            "Nov 25 17:22:21 paul-serin gnome-session-binary[2098]: WARNING: Could not parse desktop file nautilus-autostart.desktop or it references a not found TryExec binary\n",
            "Nov 25 17:22:22 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:22:29 paul-serin gnome-shell[2515]: [2509:2509:1125/172229.203746:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.ScreenSaver.GetActive: object_path= /org/freedesktop/ScreenSaver: org.freedesktop.DBus.Error.NotSupported: This method is not implemented\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2537:1125/172232.122190:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2537:1125/172232.122548:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2537:1125/172232.122613:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2537:1125/172232.122660:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2509:1125/172232.852309:ERROR:interface_endpoint_client.cc(725)] Message 1 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2509:1125/172232.852532:ERROR:interface_endpoint_client.cc(725)] Message 1 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 25 17:22:37 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:22:47 paul-serin gnome-shell[2515]: [2509:2509:1125/172247.136366:ERROR:account_info_fetcher.cc(62)] OnGetTokenFailure: Invalid credentials (credentials rejected by server).\n",
            "Nov 25 17:22:52 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:22:52 paul-serin gnome-shell[2515]: [2509:2537:1125/172252.845564:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 25 17:23:38 paul-serin gnome-shell[2515]: [2509:2537:1125/172338.434705:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 25 17:24:09 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:25:09 paul-serin gnome-shell[2515]: [2509:2537:1125/172509.535766:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 25 17:25:21 paul-serin discord_discord.desktop[5622]: [5622:1125/172521.737196:ERROR:zygote_host_impl_linux.cc(273)] Failed to adjust OOM score of renderer with pid 5783: Permission denied (13)\n",
            "Nov 25 17:25:22 paul-serin discord_discord.desktop[5810]: [5810:1125/172522.612847:ERROR:ffmpeg_common.cc(965)] Unsupported pixel format: -1\n",
            "Nov 25 17:25:24 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:25:33 paul-serin discord_discord.desktop[5622]: [5622:1125/172533.292692:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.login1.Manager.Inhibit: object_path= /org/freedesktop/login1: org.freedesktop.DBus.Error.AccessDenied: An AppArmor policy prevents this sender from sending this message to this recipient; type=\"method_call\", sender=\":1.170\" (uid=1000 pid=5622 comm=\"/snap/discord/216/usr/share/discord/Discord --use-\" label=\"snap.discord.discord (enforce)\") interface=\"org.freedesktop.login1.Manager\" member=\"Inhibit\" error name=\"(unset)\" requested_reply=\"0\" destination=\"org.freedesktop.login1\" (uid=0 pid=962 comm=\"/lib/systemd/systemd-logind \" label=\"unconfined\")\n",
            "Nov 25 17:25:36 paul-serin discord_discord.desktop[5622]: [5622:1125/172536.571434:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.Secret.Service.ReadAlias: object_path= /org/freedesktop/secrets: org.freedesktop.DBus.Error.AccessDenied: An AppArmor policy prevents this sender from sending this message to this recipient; type=\"method_call\", sender=\":1.123\" (uid=1000 pid=5622 comm=\"/snap/discord/216/usr/share/discord/Discord --use-\" label=\"snap.discord.discord (enforce)\") interface=\"org.freedesktop.Secret.Service\" member=\"ReadAlias\" error name=\"(unset)\" requested_reply=\"0\" destination=\"org.freedesktop.secrets\" (uid=1000 pid=1967 comm=\"/usr/bin/gnome-keyring-daemon --daemonize --login \" label=\"unconfined\")\n",
            "Nov 25 17:25:39 paul-serin discord_discord.desktop[5886]: [5886:1125/172539.449943:ERROR:ffmpeg_common.cc(965)] Unsupported pixel format: -1\n",
            "Nov 25 17:26:33 paul-serin discord_discord.desktop[5622]: [5622:1125/172633.190294:ERROR:atom_cache.cc(230)] Add chromium/from-privileged to kAtomsToCache\n",
            "Nov 25 17:26:39 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:26:50 paul-serin discord_discord.desktop[5622]: 17:26:50.279 › [UploaderBase.tsx] _handleError: undefined ({\"type\":\"ERROR_SOURCE_UNKNOWN\",\"msg\":\"Error: File upload9 failed to upload\"}) for Uploader11\n",
            "Nov 25 17:26:50 paul-serin discord_discord.desktop[5622]: 17:26:50.673 › [UploaderBase.tsx] _handleError: undefined ({\"type\":\"ERROR_SOURCE_UNKNOWN\",\"msg\":\"Error: Unhandled error. (undefined)\"}) for Uploader11\n",
            "Nov 25 17:27:01 paul-serin gnome-shell[2515]: [2509:2533:1125/172701.160397:ERROR:cert_verify_proc_builtin.cc(1063)] CertVerifyProcBuiltin for arel.cy-tech.fr failed:\n",
            "Nov 25 17:27:01 paul-serin gnome-shell[2515]: ERROR: Time is after notAfter\n",
            "Nov 25 17:27:01 paul-serin gnome-shell[2515]: [2571:2578:1125/172701.160792:ERROR:ssl_client_socket_impl.cc(882)] handshake failed; returned -1, SSL error code 1, net_error -201\n",
            "Nov 25 17:27:31 paul-serin discord_discord.desktop[5622]: 17:27:31.012 › [UploaderBase.tsx] _handleError: undefined ({\"type\":\"ERROR_SOURCE_UNKNOWN\",\"msg\":\"Error: File upload12 failed to upload\"}) for Uploader14\n",
            "Nov 25 17:27:31 paul-serin discord_discord.desktop[5622]: 17:27:31.352 › [UploaderBase.tsx] _handleError: undefined ({\"type\":\"ERROR_SOURCE_UNKNOWN\",\"msg\":\"Error: Unhandled error. (undefined)\"}) for Uploader14\n",
            "Nov 25 17:27:54 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:28:11 paul-serin gnome-shell[2515]: [2509:2537:1125/172811.094616:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 25 17:29:09 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:30:24 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:31:39 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:32:54 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 28 16:10:52 paul-serin ntpd[1052]: kernel reports TIME_ERROR: 0x41: Clock Unsynchronized\n",
            "Nov 28 16:10:52 paul-serin ntpd[1052]: kernel reports TIME_ERROR: 0x41: Clock Unsynchronized\n",
            "Nov 28 16:10:53 paul-serin networkd-dispatcher[1148]: WARNING: systemd-networkd is not running, output will be incomplete.\n",
            "Nov 28 16:10:53 paul-serin networkd-dispatcher[898]: ERROR:Unknown state for interface NetworkctlListState(idx=1, name='lo', type='loopback', operational='n/a', administrative='unmanaged'): n/a\n",
            "Nov 28 16:10:53 paul-serin networkd-dispatcher[898]: ERROR:Unknown state for interface NetworkctlListState(idx=2, name='wlp0s20f3', type='wlan', operational='n/a', administrative='unmanaged'): n/a\n",
            "Nov 28 16:10:56 paul-serin ovpn-cytech.students[1302]: WARNING: file '/data/CYTECH-VPN-CLE-PERSONNELLE/client.p12' is group or others accessible\n",
            "Nov 28 16:10:56 paul-serin gnome-session[1402]: gnome-session-binary[1402]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 28 16:10:56 paul-serin gnome-session-binary[1402]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 28 16:11:03 paul-serin gnome-shell[1437]: JS WARNING: [resource:///org/gnome/shell/ui/layout.js 24]: reference to undefined property \"MetaWindowXwayland\"\n",
            "Nov 28 16:11:06 paul-serin gnome-session[2040]: gnome-session-binary[2040]: WARNING: Could not parse desktop file nautilus-autostart.desktop or it references a not found TryExec binary\n",
            "Nov 28 16:11:06 paul-serin gnome-session-binary[2040]: WARNING: Could not parse desktop file nautilus-autostart.desktop or it references a not found TryExec binary\n",
            "Nov 28 16:15:03 paul-serin python3[3031]: [\"2024-11-28T16:15:03.072\", \"WARNING\", \"ubuntupro.apt\", \"fail\", 933, \"Failed to fetch ESM Apt Cache item: https://esm.ubuntu.com/apps/ubuntu/dists/focal-apps-security/InRelease\", {}]\n",
            "Nov 28 16:15:03 paul-serin python3[3031]: [\"2024-11-28T16:15:03.073\", \"WARNING\", \"ubuntupro.apt\", \"fail\", 933, \"Failed to fetch ESM Apt Cache item: https://esm.ubuntu.com/apps/ubuntu/dists/focal-apps-updates/InRelease\", {}]\n",
            "Nov 28 16:15:03 paul-serin ovpn-cytech.students[3280]: WARNING: file '/data/CYTECH-VPN-CLE-PERSONNELLE/client.p12' is group or others accessible\n",
            "Nov 28 16:15:03 paul-serin ovpn-cytech.students[3280]: WARNING: 'cipher' is present in local config but missing in remote config, local='cipher BF-CBC'\n",
            "Nov 28 16:15:04 paul-serin ovpn-cytech.students[3280]: WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this\n",
            "Nov 28 16:15:11 paul-serin gnome-shell[4576]: [4569:4569:1128/161511.443590:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.ScreenSaver.GetActive: object_path= /org/freedesktop/ScreenSaver: org.freedesktop.DBus.Error.NotSupported: This method is not implemented\n",
            "Nov 28 16:15:15 paul-serin gnome-shell[4576]: [4569:4599:1128/161515.042081:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 28 16:15:44 paul-serin gnome-shell[4576]: [4569:4599:1128/161544.363279:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 28 16:16:31 paul-serin gnome-shell[4576]: [4569:4599:1128/161631.669855:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 28 16:16:48 paul-serin gnome-shell[4576]: [4569:4599:1128/161648.902069:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 28 16:16:48 paul-serin gnome-shell[4576]: [4569:4599:1128/161648.904032:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 28 16:17:27 paul-serin ovpn-cytech.students[3280]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 28 16:17:27 paul-serin ovpn-cytech.students[3280]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 28 16:18:15 paul-serin ntpd[1073]: kernel reports TIME_ERROR: 0x41: Clock Unsynchronized\n",
            "Nov 28 16:18:15 paul-serin ntpd[1073]: kernel reports TIME_ERROR: 0x41: Clock Unsynchronized\n",
            "Nov 28 16:18:17 paul-serin ovpn-cytech.students[1155]: WARNING: file '/data/CYTECH-VPN-CLE-PERSONNELLE/client.p12' is group or others accessible\n",
            "Nov 28 16:18:17 paul-serin networkd-dispatcher[1194]: WARNING: systemd-networkd is not running, output will be incomplete.\n",
            "Nov 28 16:18:17 paul-serin networkd-dispatcher[928]: ERROR:Unknown state for interface NetworkctlListState(idx=1, name='lo', type='loopback', operational='n/a', administrative='unmanaged'): n/a\n",
            "Nov 28 16:18:17 paul-serin networkd-dispatcher[928]: ERROR:Unknown state for interface NetworkctlListState(idx=2, name='wlp0s20f3', type='wlan', operational='n/a', administrative='unmanaged'): n/a\n",
            "Nov 28 16:18:20 paul-serin gnome-session[1314]: gnome-session-binary[1314]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 28 16:18:20 paul-serin gnome-session-binary[1314]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 28 16:18:27 paul-serin gnome-shell[1346]: JS WARNING: [resource:///org/gnome/shell/ui/layout.js 24]: reference to undefined property \"MetaWindowXwayland\"\n",
            "Nov 29 09:02:48 paul-serin ntpd[1051]: kernel reports TIME_ERROR: 0x41: Clock Unsynchronized\n",
            "Nov 29 09:02:48 paul-serin ntpd[1051]: kernel reports TIME_ERROR: 0x41: Clock Unsynchronized\n",
            "Nov 29 09:02:48 paul-serin networkd-dispatcher[1117]: WARNING: systemd-networkd is not running, output will be incomplete.\n",
            "Nov 29 09:02:49 paul-serin networkd-dispatcher[905]: ERROR:Unknown state for interface NetworkctlListState(idx=1, name='lo', type='loopback', operational='n/a', administrative='unmanaged'): n/a\n",
            "Nov 29 09:02:49 paul-serin networkd-dispatcher[905]: ERROR:Unknown state for interface NetworkctlListState(idx=2, name='wlp0s20f3', type='wlan', operational='n/a', administrative='unmanaged'): n/a\n",
            "Nov 29 09:02:49 paul-serin ovpn-cytech.students[1155]: WARNING: file '/data/CYTECH-VPN-CLE-PERSONNELLE/client.p12' is group or others accessible\n",
            "Nov 29 09:02:49 paul-serin gnome-session-binary[1288]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 29 09:02:49 paul-serin gnome-session[1288]: gnome-session-binary[1288]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 29 09:02:56 paul-serin gnome-shell[1342]: JS WARNING: [resource:///org/gnome/shell/ui/layout.js 24]: reference to undefined property \"MetaWindowXwayland\"\n",
            "Nov 29 09:04:04 paul-serin gnome-session[2080]: gnome-session-binary[2080]: WARNING: Could not parse desktop file nautilus-autostart.desktop or it references a not found TryExec binary\n",
            "Nov 29 09:04:04 paul-serin gnome-session-binary[2080]: WARNING: Could not parse desktop file nautilus-autostart.desktop or it references a not found TryExec binary\n",
            "Nov 29 09:06:10 paul-serin gnome-shell[2663]: [2655:2655:1129/090610.899637:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.ScreenSaver.GetActive: object_path= /org/freedesktop/ScreenSaver: org.freedesktop.DBus.Error.NotSupported: This method is not implemented\n",
            "Nov 29 09:06:13 paul-serin gnome-shell[2663]: [2655:2685:1129/090613.778927:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 29 09:06:27 paul-serin gnome-shell[2663]: [2655:2680:1129/090627.548982:ERROR:get_updates_processor.cc(265)] PostClientToServerMessage() failed during GetUpdates with error Network error (ERR_NETWORK_CHANGED)\n",
            "Nov 29 09:06:42 paul-serin gnome-shell[2663]: [2655:2685:1129/090642.973028:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 29 09:06:45 paul-serin chrome-cifhbcnohmdccbgoicgdjpfamggdegmo-Default.desktop[5854]: [5864:5864:0100/000000.628736:ERROR:zygote_linux.cc(678)] write: Relais brisé (pipe) (32)\n",
            "Nov 29 09:06:46 paul-serin gnome-shell[2663]: [2655:2655:1129/090646.027209:ERROR:interface_endpoint_client.cc(725)] Message 0 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 09:07:14 paul-serin gnome-shell[2663]: [2655:2655:1129/090714.756184:ERROR:atom_cache.cc(229)] Add WM_CHANGE_STATE to kAtomsToCache\n",
            "Nov 29 09:07:20 paul-serin ovpn-cytech.students[1155]: WARNING: 'cipher' is present in local config but missing in remote config, local='cipher BF-CBC'\n",
            "Nov 29 09:07:21 paul-serin ovpn-cytech.students[1155]: WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this\n",
            "Nov 29 09:07:33 paul-serin gnome-shell[2663]: [2655:2685:1129/090733.134781:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 29 09:08:14 paul-serin gnome-shell[2663]: [2655:2655:1129/090814.766467:ERROR:interface_endpoint_client.cc(725)] Message 1 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 09:09:24 paul-serin gnome-shell[2663]: [2655:2685:1129/090924.165695:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 29 09:13:11 paul-serin gnome-shell[2663]: [2655:2685:1129/091311.213496:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 29 09:18:10 paul-serin discord_discord.desktop[8157]: [8157:1129/091810.981949:ERROR:zygote_host_impl_linux.cc(273)] Failed to adjust OOM score of renderer with pid 8315: Permission denied (13)\n",
            "Nov 29 09:18:11 paul-serin discord_discord.desktop[8335]: [8335:1129/091811.870721:ERROR:ffmpeg_common.cc(965)] Unsupported pixel format: -1\n",
            "Nov 29 09:18:24 paul-serin discord_discord.desktop[8157]: [8157:1129/091824.255617:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.login1.Manager.Inhibit: object_path= /org/freedesktop/login1: org.freedesktop.DBus.Error.AccessDenied: An AppArmor policy prevents this sender from sending this message to this recipient; type=\"method_call\", sender=\":1.193\" (uid=1000 pid=8157 comm=\"/snap/discord/218/usr/share/discord/Discord --use-\" label=\"snap.discord.discord (enforce)\") interface=\"org.freedesktop.login1.Manager\" member=\"Inhibit\" error name=\"(unset)\" requested_reply=\"0\" destination=\"org.freedesktop.login1\" (uid=0 pid=934 comm=\"/lib/systemd/systemd-logind \" label=\"unconfined\")\n",
            "Nov 29 09:18:29 paul-serin discord_discord.desktop[8157]: [8157:1129/091829.661606:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.Secret.Service.ReadAlias: object_path= /org/freedesktop/secrets: org.freedesktop.DBus.Error.AccessDenied: An AppArmor policy prevents this sender from sending this message to this recipient; type=\"method_call\", sender=\":1.125\" (uid=1000 pid=8157 comm=\"/snap/discord/218/usr/share/discord/Discord --use-\" label=\"snap.discord.discord (enforce)\") interface=\"org.freedesktop.Secret.Service\" member=\"ReadAlias\" error name=\"(unset)\" requested_reply=\"0\" destination=\"org.freedesktop.secrets\" (uid=1000 pid=1949 comm=\"/usr/bin/gnome-keyring-daemon --daemonize --login \" label=\"unconfined\")\n",
            "Nov 29 09:19:12 paul-serin gnome-shell[2663]: [2655:2685:1129/091912.792115:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 29 09:19:34 paul-serin gnome-shell[2663]: [2655:2685:1129/091934.245716:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 09:19:34 paul-serin gnome-shell[2663]: [2655:2685:1129/091934.247352:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 09:20:03 paul-serin gnome-shell[2663]: [2655:2685:1129/092003.573584:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 09:20:14 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 09:20:14 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 09:20:14 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 09:20:14 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 09:20:14 paul-serin gnome-shell[2663]: [2655:2685:1129/092014.132960:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 09:40:52 paul-serin discord_discord.desktop[8157]: [8157:1129/094052.137733:ERROR:atom_cache.cc(230)] Add chromium/from-privileged to kAtomsToCache\n",
            "Nov 29 09:56:32 paul-serin ovpn-cytech.students[1155]: WARNING: 'cipher' is present in local config but missing in remote config, local='cipher BF-CBC'\n",
            "Nov 29 10:09:48 paul-serin gnome-shell[2663]: [2655:2655:1129/100948.132239:ERROR:interface_endpoint_client.cc(725)] Message 0 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 10:21:39 paul-serin gnome-shell[2663]: [2655:2655:1129/102139.824328:ERROR:interface_endpoint_client.cc(725)] Message 0 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 10:23:16 paul-serin gnome-shell[2663]: [2655:2685:1129/102316.081919:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 10:23:26 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 10:23:26 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 10:23:26 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 10:23:26 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 10:23:26 paul-serin ovpn-cytech.students[1155]: ERROR: Linux route delete command failed: external program exited with error status: 2\n",
            "Nov 29 10:23:26 paul-serin gnome-shell[2663]: [2655:2685:1129/102326.210775:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 11:47:55 paul-serin gnome-shell[2663]: [2655:2655:1129/114755.782232:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 11:48:12 paul-serin gnome-shell[2663]: [2655:2655:1129/114812.913131:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 12:07:51 paul-serin gnome-shell[2663]: [2655:2685:1129/120751.790691:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 12:07:56 paul-serin gnome-shell[2663]: [2655:2685:1129/120756.787980:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 12:13:52 paul-serin gnome-shell[2663]: [2655:2655:1129/121352.829598:ERROR:interface_endpoint_client.cc(725)] Message 0 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 12:14:09 paul-serin gnome-shell[2663]: [2655:2685:1129/121409.778151:ERROR:connection_factory_impl.cc(483)] ConnectionHandler failed with net error: -2\n",
            "Nov 29 14:03:07 paul-serin gnome-shell[2663]: [2717:2723:1129/140307.778554:ERROR:ssl_client_socket_impl.cc(882)] handshake failed; returned -1, SSL error code 1, net_error -101\n",
            "Nov 29 14:09:05 paul-serin gnome-shell[2663]: [2655:2685:1129/140905.941979:ERROR:connection_factory_impl.cc(434)] Failed to connect to MCS endpoint with error -105\n",
            "Nov 29 14:33:02 paul-serin gnome-shell[2663]: [2655:2655:1129/143302.361930:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 14:39:24 paul-serin gnome-shell[2663]: [2655:2655:1129/143924.334731:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 15:14:22 paul-serin gnome-shell[2663]: [2655:2655:1129/151422.671612:ERROR:atom_cache.cc(229)] Add chromium/from-privileged to kAtomsToCache\n",
            "Nov 29 15:16:58 paul-serin gnome-shell[2663]: [2655:2655:1129/151658.887127:ERROR:interface_endpoint_client.cc(725)] Message 0 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 15:25:01 paul-serin gnome-shell[2663]: [2655:2655:1129/152501.600567:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 15:30:08 paul-serin gnome-shell[2663]: [2655:2655:1129/153008.051842:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 15:30:25 paul-serin gnome-shell[2663]: [2655:2655:1129/153025.486013:ERROR:interface_endpoint_client.cc(725)] Message 0 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 15:31:55 paul-serin gnome-shell[2663]: [2655:2655:1129/153155.941074:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 15:50:05 paul-serin gnome-shell[2663]: [2655:2655:1129/155005.331683:ERROR:interface_endpoint_client.cc(725)] Message 0 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 15:52:02 paul-serin gnome-shell[2663]: [2655:2655:1129/155202.277815:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 15:52:45 paul-serin gnome-shell[2663]: [2655:2655:1129/155245.766946:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 15:53:33 paul-serin gnome-shell[2663]: [2655:2655:1129/155333.851767:ERROR:interface_endpoint_client.cc(725)] Message 2 rejected by interface blink.mojom.Widget\n",
            "Nov 29 16:16:46 paul-serin gnome-shell[2663]: [2655:2655:1129/161646.702790:ERROR:interface_endpoint_client.cc(725)] Message 0 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 29 16:20:06 paul-serin gnome-shell[2663]: [2702:2702:1129/162006.867419:ERROR:shared_image_manager.cc(248)] SharedImageManager::ProduceSkia: Trying to Produce a Skia representation from a non-existent mailbox.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Assuming a SparkSession is already active\n",
        "# If not, uncomment the line below to initialize it\n",
        "spark = SparkSession.builder.appName(\"Inspect Log File\").getOrCreate()\n",
        "\n",
        "# Step 2: Load the syslog file\n",
        "log_file_path = \"/content/syslog\"  # Replace with the uploaded file path\n",
        "rdd = spark.sparkContext.textFile(log_file_path)\n",
        "\n",
        "# Step 3: Filter lines containing \"WARNING\" or \"ERROR\"\n",
        "bad_lines = rdd.filter(lambda line: \"WARNING\" in line or \"ERROR\" in line)\n",
        "\n",
        "# Step 4: Collect and display the bad lines\n",
        "bad_lines_collected = bad_lines.collect()\n",
        "print(\"Bad lines (WARNING and ERROR messages):\")\n",
        "for line in bad_lines_collected:\n",
        "    print(line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataframe approach"
      ],
      "metadata": {
        "id": "dRBstjBoKYNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, lit\n",
        "\n",
        "# Step 1: Load the syslog file into a DataFrame\n",
        "df_logs = spark.read.text(log_file_path).withColumnRenamed(\"value\", \"line\")\n",
        "\n",
        "# Step 2: Add a column to classify lines as \"bad\" or \"good\"\n",
        "# A \"bad\" line contains \"WARNING\" or \"ERROR\", otherwise it is \"good\"\n",
        "df_logs = df_logs.withColumn(\n",
        "    \"is_bad\",\n",
        "    when((col(\"line\").contains(\"WARNING\")) | (col(\"line\").contains(\"ERROR\")), lit(1)).otherwise(lit(0))\n",
        ")\n",
        "\n",
        "# Step 3: Count the number of bad and good lines\n",
        "bad_lines_count = df_logs.filter(col(\"is_bad\") == 1).count()\n",
        "good_lines_count = df_logs.filter(col(\"is_bad\") == 0).count()\n",
        "\n",
        "# Step 4: Show results\n",
        "print(f\"Number of bad lines (WARNING or ERROR): {bad_lines_count}\")\n",
        "print(f\"Number of good lines: {good_lines_count}\")\n"
      ],
      "metadata": {
        "id": "S7OoWtMlKTls",
        "outputId": "09d134c3-b130-42ae-eca3-8d5d97fc11ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bad lines (WARNING or ERROR): 146\n",
            "Number of good lines: 107068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ708zvl627q"
      },
      "source": [
        "## **Exercise 5.1: Word count**\n",
        "\n",
        "**Using RDDs**, count the number of lines in the `$DRIVE_DATA/quijote.txt` file. Then, count the number of words in the file. Finally, count the number of *different* words in the file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Slr4nXhp_knl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2afbac85-36c3-4b89-f7db-e0927bbee663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 31931\n",
            "Number of words: 387834\n",
            "Number of different words: 40060\n"
          ]
        }
      ],
      "source": [
        "file_path = f\"{os.environ['DRIVE_DATA']}quijote.txt\"\n",
        "\n",
        "rdd = spark.sparkContext.textFile(file_path)\n",
        "\n",
        "rdd = rdd.filter(lambda line: line.strip() != \"\") # We remove empty lines\n",
        "\n",
        "num_lines = rdd.count()\n",
        "\n",
        "num_words = rdd.flatMap(lambda line: line.split(\" \")).count()\n",
        "\n",
        "num_distinct_words = rdd.flatMap(lambda line: line.split(\" \")).distinct().count()\n",
        "\n",
        "print(f\"Number of lines: {num_lines}\")\n",
        "print(f\"Number of words: {num_words}\")\n",
        "print(f\"Number of different words: {num_distinct_words}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjtVkYOKxTo7"
      },
      "source": [
        "## **Exercise 5.2: Count people by age**\n",
        "\n",
        "Using RDDs, create a barplot showing of number of people (y-axis) per age (x-axis) using the information in the $DRIVE_DATA/people.txt file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-JgE0XavBBLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "7bb07c67-3031-48e2-8385-ad8da2e49d3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = f\"{os.environ['DRIVE_DATA']}people.txt\"\n",
        "\n",
        "rdd = spark.sparkContext.textFile(file_path)\n",
        "\n",
        "# Extract age and count occurrences\n",
        "age_counts = rdd.map(lambda line: line.split(\"\\t\")[1])\n",
        "age_counts = age_counts.map(lambda age: (age, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Collect the results\n",
        "age_counts_data = age_counts.collect()\n",
        "\n",
        "# Sort for plotting\n",
        "ages, counts = zip(*sorted(age_counts_data, key=lambda x: int(x[0])))\n",
        "\n",
        "# Create the bar plot\n",
        "plt.bar(ages, counts)\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Number of People\")\n",
        "plt.title(\"Number of People per Age\")\n",
        "#plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xddDZ2zCxTor"
      },
      "source": [
        "### **Exercise 5.3: Obtain the number of received citations**\n",
        "\n",
        "Using RDDs, write a PySpark program that obtains, from the cite75_99.txt file, the number of citations received by each patent.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "mwUQVKrY4a-T"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.textFile(os.environ[\"DRIVE_DATA\"] + \"/cite75_99.txt.tar.bz2\")\n",
        "\n",
        "rdd.take(10)\n",
        "\n",
        "citations = (\n",
        "    rdd.map(lambda line: line.split(\",\")[1])  # Extract cited patent ID\n",
        "       .map(lambda patent: (patent, 1))      # Create key-value pairs (patent, 1)\n",
        "       .reduceByKey(lambda x, y: x + y)      # Sum up counts for each patent\n",
        ")\n",
        "\n",
        "citations_data = citations.collect()\n",
        "print(\"Number of citations received by each patent:\")\n",
        "for patent, count in sorted(citations_data, key=lambda x: -x[1]):  # Sort by count descending\n",
        "    print(f\"Patent {patent}: {count} citations\")"
      ],
      "metadata": {
        "id": "QvMWJc0Sj_6l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2e0556b6-4393-4bf5-a572-1719ff0ed427"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 1 times, most recent failure: Lost task 2.0 in stage 14.0 (TID 23) (2dc0fbab3006 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-29-a0bec5cf0fbf>\", line 6, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-29-a0bec5cf0fbf>\", line 6, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-a0bec5cf0fbf>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcitations_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcitations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of citations received by each patent:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitations_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Sort by count descending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 14.0 failed 1 times, most recent failure: Lost task 2.0 in stage 14.0 (TID 23) (2dc0fbab3006 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-29-a0bec5cf0fbf>\", line 6, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-29-a0bec5cf0fbf>\", line 6, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the file into an RDD\n",
        "rdd = sc.textFile(os.environ[\"DRIVE_DATA\"] + \"/cite75_99.txt.tar.bz2\")\n",
        "\n",
        "rdd.take(100000)\n",
        "\n",
        "# Step 2: Filter rows with a valid format\n",
        "# Ensure the line contains at least one comma and has two elements when split\n",
        "valid_rows = rdd.filter(lambda line: \",\" in line and len(line.split(\",\")) >= 2)\n",
        "\n",
        "# Step 3: Extract cited patents (second column)\n",
        "citations = valid_rows.map(lambda line: line.split(\",\")[1].strip())  # Extract the cited patent ID\n",
        "\n",
        "# Step 4: Count citations for each patent\n",
        "citations_count = citations.map(lambda patent: (patent, 1)) \\\n",
        "                           .reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Step 5: Collect and display results\n",
        "citations_data = citations_count.collect()\n",
        "print(\"Number of citations received by each patent (Top 1000):\")\n",
        "for patent, count in sorted(citations_data, key=lambda x: -x[1])[:1000]:\n",
        "    print(f\"Patent {patent}: {count} citations\")\n"
      ],
      "metadata": {
        "id": "K8zv0pBdYJpo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usL5LynSI6rE"
      },
      "source": [
        "## Exercise 11.1:\n",
        "\n",
        "A long time ago in a galaxy far, far away, the characters of the Star Wars franchise interacted with each other in an endless series of films. An ancient Jedi order, called the *Data Guardians of the Galaxy* (not affiliated to Marvel's homonym :) registered all those interactions and saved them on a digital file so that they could be studied by the forthcoming generations. This file was originally called (guess it) `sw.txt`, and you will find it in the `/data` directory.\n",
        "\n",
        "Using pySpark, perform the following operations and answer the following questions:\n",
        "\n",
        "1. Load the `$DRIVE_DATA/sw.txt` file. Take into account that it is a JSON file.\n",
        "2. Using this information, create a graph of interactions between the Star Wars characters.\n",
        "3. How many different characters are there?\n",
        "4. How many interactions are there?\n",
        "5. Who is the central character in Star Wars (the one who interacts in most scenes)?\n",
        "6. Who is the character with the highest 'rank' in Star Wars (use the PageRank algorithm)?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1 : Load the $DRIVE_DATA/sw.txt file. Take into account that it is a JSON file.*"
      ],
      "metadata": {
        "id": "H1g0sHbsKLSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import MULTILINE\n",
        "sw_DF = spark.read.json(os.environ[\"DRIVE_DATA\"] + \"sw.txt\", multiLine=True).cache()\n",
        "sw_DF.printSchema()\n",
        "sw_DF.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "rtmqUXYSKQZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2 : Using this information, create a graph of interactions between the Star Wars characters.*"
      ],
      "metadata": {
        "id": "QEv2n-qkKdjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import posexplode\n",
        "\n",
        "# Explode the 'nodes' array with positions to get IDs\n",
        "nodes_DF = sw_DF.select(posexplode(\"nodes\").alias(\"id\", \"node\"))\n",
        "\n",
        "# Flatten the nodes DataFrame\n",
        "nodes_DF = nodes_DF.selectExpr(\"id\", \"node.name as name\", \"node.value as interactions\", \"node.colour as colour\")\n",
        "\n",
        "# Create a temporary view for SQL queries\n",
        "nodes_DF.createOrReplaceTempView(\"nodes\")\n",
        "\n",
        "nodes_DF.show(truncate=False)"
      ],
      "metadata": {
        "id": "EXCES8npMUN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode the 'links' array\n",
        "links_DF = sw_DF.selectExpr(\"explode(links) as link\")\n",
        "\n",
        "# Flatten the links DataFrame\n",
        "links_DF = links_DF.selectExpr(\"link.source as source\", \"link.target as target\", \"link.value as value\")\n",
        "\n",
        "links_DF.createOrReplaceTempView(\"links\")\n",
        "\n",
        "links_DF.show(truncate=False)"
      ],
      "metadata": {
        "id": "lyg-VVDLRhM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2Xn4OMhnzxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3. How many different characters are there?*"
      ],
      "metadata": {
        "id": "xyVrMGqOKds4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_characters = spark.sql(\"SELECT COUNT(DISTINCT name) AS num_characters FROM nodes\").collect()[0][0]\n",
        "print(f\"Number of different characters: {num_characters}\")\n"
      ],
      "metadata": {
        "id": "k2oR8plxR_q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*4. How many interactions are there?*"
      ],
      "metadata": {
        "id": "JBo7BVJnKdzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_interactions = spark.sql(\"SELECT COUNT(*) AS num_interactions FROM links\").collect()[0][0]\n",
        "print(f\"Number of interactions: {num_interactions}\")"
      ],
      "metadata": {
        "id": "h4IzajgvRs6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5. Who is the central character in Star Wars (the one who interacts in most scenes)?*"
      ],
      "metadata": {
        "id": "pBSgqhCsKeQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of outgoing interactions for each source\n",
        "out_degree_DF = spark.sql(\"SELECT source AS id, COUNT(*) AS out_degree FROM links GROUP BY source\")\n",
        "\n",
        "# Create a temporary view for further SQL queries\n",
        "out_degree_DF.createOrReplaceTempView(\"out_degree\")\n",
        "\n",
        "# Show the results\n",
        "out_degree_DF.show(truncate=False)"
      ],
      "metadata": {
        "id": "rvej0gYGNnsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of incoming interactions for each target\n",
        "in_degree_DF = spark.sql(\"SELECT target AS id, COUNT(*) AS in_degree FROM links GROUP BY target\")\n",
        "\n",
        "# Create a temporary view for further SQL queries\n",
        "in_degree_DF.createOrReplaceTempView(\"in_degree\")\n",
        "\n",
        "# Show the results\n",
        "in_degree_DF.show(truncate=False)"
      ],
      "metadata": {
        "id": "fZaXnNF4ec4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the SQL query\n",
        "central_character_DF = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    nodes.name,\n",
        "    COALESCE(out_degree.out_degree, 0) + COALESCE(in_degree.in_degree, 0) AS total_interactions\n",
        "FROM\n",
        "    nodes\n",
        "LEFT JOIN\n",
        "    out_degree ON nodes.id = out_degree.id\n",
        "LEFT JOIN\n",
        "    in_degree ON nodes.id = in_degree.id\n",
        "ORDER BY\n",
        "    total_interactions DESC\n",
        "\"\"\")\n",
        "\n",
        "# Show the top character\n",
        "central_character = central_character_DF.first()\n",
        "print(f\"The central character is {central_character['name']} with {central_character['total_interactions']} interactions.\")\n"
      ],
      "metadata": {
        "id": "KrS73jlsegdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*6. Who is the character with the highest 'rank' in Star Wars (use the PageRank algorithm)?*"
      ],
      "metadata": {
        "id": "MKLs9VNQKwZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize RDDs\n",
        "edges_rdd = links_DF.rdd.map(lambda row: (row[\"source\"], row[\"target\"]))\n",
        "vertices_rdd = nodes_DF.rdd.map(lambda row: (row[\"id\"], row[\"name\"]))\n",
        "\n",
        "# Create adjacency list\n",
        "links_rdd = edges_rdd.groupByKey().mapValues(list)\n",
        "\n",
        "# Initialize ranks\n",
        "ranks_rdd = vertices_rdd.mapValues(lambda _: 1.0)\n",
        "\n",
        "# Run PageRank for 10 iterations\n",
        "for _ in range(10):\n",
        "    contributions = links_rdd.join(ranks_rdd).flatMap(\n",
        "        lambda x: [(target, x[1][1] / len(x[1][0])) for target in x[1][0]]\n",
        "    )\n",
        "    ranks_rdd = contributions.reduceByKey(lambda x, y: x + y).mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
        "\n",
        "# Combine with character names and sort by rank\n",
        "pagerank_results = ranks_rdd.join(vertices_rdd).map(lambda x: (x[1][1], x[1][0])).sortBy(lambda x: -x[1])\n",
        "\n",
        "# Display the results\n",
        "print(\"PageRank results:\")\n",
        "for name, rank in pagerank_results.collect():\n",
        "    print(f\"{name}: {rank:.4f}\")\n",
        "\n",
        "# Find the character with the highest rank\n",
        "most_ranked_character = pagerank_results.take(1)[0]\n",
        "print(f\"The character with the highest rank is {most_ranked_character[0]} with a PageRank score of {most_ranked_character[1]:.4f}.\")"
      ],
      "metadata": {
        "id": "8i7w2Ryuf9Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuJnt6Y1lcUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 12 - Exercises. Final assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za4qzjHXyxcn"
      },
      "source": [
        "## Exercise 12.1\n",
        "\n",
        "Let us extract information from the cite75_99.txt and apat63_99.txt files. Write a script that performs the following operations:\n",
        "\n",
        "1. From the cite75_99.txt file, obtain the number of citations received by each patent. You must produce a DataFrame with the following format:\n",
        "\n",
        "| PatentNum | ncitations |\n",
        "|-----------|------------|\n",
        "| 3060453   |    3       |\n",
        "| 3390168   |    6       |\n",
        "| 3626542   |   18       |\n",
        "| 3611507   |    5       |\n",
        "| 3000113   |    4       |\n",
        "\n",
        "\n",
        "2. From the apat63_99.txt file, create a DataFrame to show the patent number, its country and the patent year, discarding the rest of fields in the file. The DataFrame produced must have the following format:\n",
        "\n",
        "|PatentNum |country|Year |\n",
        "|----------|-------|-----|\n",
        "| 3070801  | BE    | 1963|\n",
        "| 3070802  | US    | 1963|\n",
        "| 3070803  | US    | 1963|\n",
        "| 3070804  | US    | 1963|\n",
        "| 3070805  | US    | 1963|\n",
        "\n",
        "\n",
        "**Requirements**\n",
        "\n",
        " - Both DataFrames must be stored in Parquet format with gzip compression. Check the number of partitions of each DataFrame and the number of files gererated.\n",
        "\n",
        " - It is **strongly advised** to copy the files from your Drive to a temporal directory in the notebook virtual machine and unzip them there. This will reduce the execution times. See the cell below:\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV_M6xMlB9hP"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the cite75_99.txt file as a text DataFrame\n",
        "cite_path = os.environ[\"DRIVE_DATA\"] + \"cite75_99.txt\"\n",
        "cite_df = spark.read.text(cite_path)\n",
        "\n",
        "# Display raw data\n",
        "print(\"Raw data:\")\n",
        "cite_df.show(5, truncate=False)\n",
        "\n",
        "# Step 1: Filter out header and split into Citing and Cited columns\n",
        "cite_split_df = (\n",
        "    cite_df.filter(~F.col(\"value\").contains(\"CITING\"))  # Remove the header\n",
        "    .withColumn(\"Citing\", F.split(F.col(\"value\"), \",\").getItem(0).cast(\"long\"))\n",
        "    .withColumn(\"Cited\", F.split(F.col(\"value\"), \",\").getItem(1).cast(\"long\"))\n",
        "    .drop(\"value\")\n",
        ")\n",
        "\n",
        "# Display split data\n",
        "print(\"Split data:\")\n",
        "cite_split_df.show(5, truncate=False)\n",
        "\n",
        "cite_split_df.createOrReplaceTempView(\"cite_data\")\n",
        "\n",
        "# Use SQL to compute citation counts\n",
        "citations_sql_df = spark.sql(\"\"\"\n",
        "    SELECT Cited AS PatentNum, COUNT(*) AS ncitations\n",
        "    FROM cite_data\n",
        "    GROUP BY Cited\n",
        "    ORDER BY Cited\n",
        "\"\"\")\n",
        "\n",
        "# Show SQL results\n",
        "print(\"Citations count (SQL):\")\n",
        "citations_sql_df.show(5, truncate=False)\n",
        "\n",
        "# Save SQL results as Parquet with gzip compression\n",
        "output_path_citations_sql = \"/tmp/data/citations_count_sql.parquet\"\n",
        "citations_sql_df.write.parquet(output_path_citations_sql, compression=\"gzip\", mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved Parquet files\n",
        "print(f\"Number of partitions (SQL): {citations_sql_df.rdd.getNumPartitions()}\")\n",
        "!ls -lh {output_path_citations_sql}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the apat63_99.txt file\n",
        "apat_path = os.environ[\"DRIVE_DATA\"] + \"apat63_99.txt\"\n",
        "\n",
        "# Specify the correct delimiter and header\n",
        "apat_df = spark.read.csv(apat_path, sep=\",\", header=True, inferSchema=True)\n",
        "\n",
        "# Show the raw DataFrame to confirm correct loading\n",
        "apat_df.show(5, truncate=False)\n",
        "\n",
        "# Select relevant columns and rename them\n",
        "apat_selected_df = (\n",
        "    apat_df.select(F.col(\"PATENT\").alias(\"PatentNum\"),\n",
        "                   F.col(\"COUNTRY\").alias(\"country\"),\n",
        "                   F.col(\"GYEAR\").alias(\"Year\"))\n",
        ")\n",
        "\n",
        "# Show the selected columns\n",
        "apat_selected_df.show(5, truncate=False)\n",
        "\n",
        "# Save as Parquet with gzip compression\n",
        "output_path_apat = \"/tmp/data/apat_selected.parquet\"\n",
        "apat_selected_df.write.parquet(output_path_apat, compression=\"gzip\", mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved Parquet files\n",
        "print(f\"Number of partitions: {apat_selected_df.rdd.getNumPartitions()}\")\n",
        "!ls -lh {output_path_apat}"
      ],
      "metadata": {
        "id": "8qnA4a7aSkyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF1_LXZEnzMn"
      },
      "source": [
        "## Exercise 12.2\n",
        "\n",
        "Write a code that, from the Parquet files created in the previous exercise, obtains for each country and for each year: the total number of patents, the total number of citations from those patents, the average number of citations and the maximum number of citations. Compute only those values in which there are any values in both files (*inner join*). In addition, each country must show its whole name, obtained from the *country_codes.txt* file. The final DataFrame must look like this one:\n",
        "\n",
        "\n",
        "|Country            |Year|PatentsNum |TotalCitations|AvgCitations      |MaxCitations|\n",
        "|-------------------|----|-----------|--------------|------------------|------------|\n",
        "|Algeria            |1963|2          |7             |3.5               |4           |\n",
        "|Algeria            |1968|1          |2             |2.0               |2           |\n",
        "|Algeria            |1970|1          |2             |2.0               |2           |\n",
        "|Algeria            |1972|1          |1             |1.0               |1           |\n",
        "|Algeria            |1977|1          |2             |2.0               |2           |\n",
        "|Andorra            |1987|1          |3             |3.0               |3           |\n",
        "|Andorra            |1993|1          |1             |1.0               |1           |\n",
        "|Andorra            |1998|1          |1             |1.0               |1           |\n",
        "|Antigua and Barbuda|1978|1          |6             |6.0               |6           |\n",
        "|Antigua and Barbuda|1979|1          |14            |14.0              |14          |\n",
        "|Antigua and Barbuda|1991|1          |8             |8.0               |8           |\n",
        "|Antigua and Barbuda|1994|1          |19            |19.0              |19          |\n",
        "|Antigua and Barbuda|1995|2          |12            |6.0               |11          |\n",
        "|Antigua and Barbuda|1996|2          |3             |1.5               |2           |\n",
        "|Argentina          |1963|14         |35            |2.5               |7           |\n",
        "|Argentina          |1964|20         |60            |3.0               |8           |\n",
        "|Argentina          |1965|10         |35            |3.5               |10          |\n",
        "|Argentina          |1966|16         |44            |2.75              |9           |\n",
        "|Argentina          |1967|13         |60            |4.615384615384615 |14          |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The output DataFrame must be saved in a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the Parquet files\n",
        "citations_df = spark.read.parquet(\"/tmp/data/citations_count.parquet\")\n",
        "patents_df = spark.read.parquet(\"/tmp/data/apat_selected.parquet\")\n",
        "\n",
        "# Show the loaded data\n",
        "print(\"Citations DataFrame:\")\n",
        "citations_df.show(5, truncate=False)\n",
        "\n",
        "print(\"Patents DataFrame:\")\n",
        "patents_df.show(5, truncate=False)\n",
        "\n",
        "# Inner join between patents and citations using PatentNum\n",
        "joined_df = patents_df.join(\n",
        "    citations_df,\n",
        "    patents_df.PatentNum == citations_df.Cited,\n",
        "    how=\"inner\"\n",
        ").drop(\"Cited\")  # Drop redundant column\n",
        "\n",
        "# Group and aggregate\n",
        "aggregated_df = joined_df.groupBy(\"country\", \"Year\").agg(\n",
        "    F.count(\"PatentNum\").alias(\"PatentsNum\"),\n",
        "    F.sum(\"ncitations\").alias(\"TotalCitations\"),\n",
        "    F.avg(\"ncitations\").alias(\"AvgCitations\"),\n",
        "    F.max(\"ncitations\").alias(\"MaxCitations\")\n",
        ")\n",
        "\n",
        "# Load the country codes file with the correct separator (tab-separated)\n",
        "country_codes_path = os.environ[\"DRIVE_DATA\"] + \"country_codes.txt\"\n",
        "country_codes_df = spark.read.csv(country_codes_path, sep=\"\\t\", header=False, inferSchema=True)\n",
        "\n",
        "# Rename columns for clarity\n",
        "country_codes_df = country_codes_df.withColumnRenamed(\"_c0\", \"code\").withColumnRenamed(\"_c1\", \"name\")\n",
        "\n",
        "# Verify the schema of country_codes_df\n",
        "print(\"Country Codes DataFrame:\")\n",
        "country_codes_df.show(5, truncate=False)\n",
        "\n",
        "# Join to replace country codes with full names and sort by Country and Year\n",
        "final_df = aggregated_df.join(\n",
        "    country_codes_df,\n",
        "    aggregated_df.country == country_codes_df.code,\n",
        "    how=\"inner\"\n",
        ").select(\n",
        "    F.col(\"name\").alias(\"Country\"),\n",
        "    \"Year\",\n",
        "    \"PatentsNum\",\n",
        "    \"TotalCitations\",\n",
        "    \"AvgCitations\",\n",
        "    \"MaxCitations\"\n",
        ").orderBy(\"Country\", \"Year\")  # Sort by Country (alphabetical) and Year (ascending)\n",
        "\n",
        "# Show the final DataFrame\n",
        "print(\"Final DataFrame (sorted):\")\n",
        "final_df.show(10, truncate=False)\n",
        "\n",
        "# Save the final DataFrame as a single CSV file\n",
        "output_path = \"/tmp/data/final_patents_citations.csv\"\n",
        "final_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved CSV file\n",
        "print(\"Saved CSV File:\")\n",
        "!ls -lh {output_path}\n"
      ],
      "metadata": {
        "id": "dxyt7LAWUTQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT3WU-1IwOPD"
      },
      "source": [
        "## Exercise 12.3\n",
        "\n",
        "From the apat63_99.txt file, obtain the number of patents per country and year **using RDDs** (do not use DataFrames). The resulting RDD must be a key/value RDD in which the key is a country and the value a list of tuples. Each tuple will be composed of a year and the number of patents of the country during that year. In addition, the resulting RDD must be sorted by  the country code and, for each country, values must be sorted by year.\n",
        "\n",
        "Example of output key/value entry:\n",
        "\n",
        "    (u'PA', [(u'1963', 2), (u'1964', 2), (u'1965', 1), (u'1966', 1), (u'1970', 1), (u'1971', 1), (u'1972', 6), (u'1974', 3), (u'1975', 5), (u'1976', 3), (u'1977', 2), (u'1978', 2), (u'1980', 2), (u'1982', 1), (u'1983', 1), (u'1985', 2), (u'1986', 1), (u'1987', 2), (u'1988', 1), (u'1990', 1), (u'1991', 2), (u'1993', 1), (u'1995', 1), (u'1996', 1), (u'1999', 1)])\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- You must remove the double quotation marks from the country code.\n",
        "- Use 8 partitions to read the apat63_99.txt.bz2 file.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "apat_rdd = spark.sparkContext.textFile(os.environ[\"DRIVE_DATA\"] + \"apat63_99.txt\", minPartitions=8)\n",
        "\n",
        "# Remove the header row\n",
        "header = apat_rdd.first()\n",
        "print(f\"Header: {header}\")\n",
        "apat_rdd = apat_rdd.filter(lambda line: line != header)\n",
        "print(\"First 5 lines after removing header:\")\n",
        "print(apat_rdd.take(5))\n",
        "\n",
        "# Parse the data and handle malformed lines\n",
        "def parse_line(line):\n",
        "    fields = line.split(\",\")\n",
        "    if len(fields) > 4:  # Ensure there are enough fields\n",
        "        country = fields[4].replace('\"', '')  # Remove double quotes from the country code\n",
        "        year = fields[1]\n",
        "        return country, year\n",
        "    else:\n",
        "        return None  # Return None for malformed lines\n",
        "\n",
        "# Apply parsing and filter out invalid rows\n",
        "parsed_rdd = apat_rdd.map(parse_line)\n",
        "print(\"First 5 parsed lines (including None for malformed):\")\n",
        "print(parsed_rdd.take(5))\n",
        "\n",
        "parsed_rdd = parsed_rdd.filter(lambda x: x is not None)\n",
        "print(\"First 5 valid parsed lines:\")\n",
        "print(parsed_rdd.take(5))\n",
        "\n",
        "# Map to ((country, year), 1), reduce to count patents per (country, year)\n",
        "country_year_rdd = (\n",
        "    parsed_rdd.map(lambda pair: ((pair[0], pair[1]), 1))  # Map to ((country, year), 1)\n",
        "    .reduceByKey(lambda x, y: x + y)  # Count patents per (country, year)\n",
        ")\n",
        "\n",
        "print(\"First 5 entries after reduceByKey:\")\n",
        "print(country_year_rdd.take(5))  # Print reduced key-value pairs\n",
        "\n",
        "# Transform to (country, (year, count)) and group by country\n",
        "grouped_rdd = (\n",
        "    country_year_rdd.map(lambda pair: (pair[0][0], (pair[0][1], pair[1])))  # Transform to (country, (year, count))\n",
        "    .groupByKey()  # Group by country\n",
        ")\n",
        "\n",
        "print(\"First 5 grouped entries by country:\")\n",
        "print(grouped_rdd.mapValues(list).take(5))  # Print grouped entries (convert iterator to list for printing)\n",
        "\n",
        "# Sort by year for each country and then by country code\n",
        "sorted_rdd = (\n",
        "    grouped_rdd.mapValues(lambda years: sorted(list(years), key=lambda x: x[0]))  # Sort by year for each country\n",
        "    .sortByKey()\n",
        ")\n",
        "\n",
        "result = sorted_rdd.collect()\n",
        "print(\"Final result (first 5 entries):\")\n",
        "print(result[:5])\n"
      ],
      "metadata": {
        "id": "5MVD0qfWWZmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0LiWoc4VQdh"
      },
      "source": [
        "## Exercise 12.4\n",
        "\n",
        "From the Parquet files created in Exercise 12.1, create a DataFrame that gives the patent or patents with the higher number of citations per country and year, as well as the average of the number of citations per country and year, and the difference between the maximum and the average values. The resulting DataFrame should look like this:\n",
        "\n",
        "\n",
        "|Country|Year|PatentNum|max  |average       |diff              |\n",
        "|-------|----|---------|-----|--------------|------------------|\n",
        "|AD     |1987|4688621  |3    |3.0           |0.0               |\n",
        "|AD     |1993|5193231  |1    |1.0           |0.0               |\n",
        "|AD     |1998|5765303  |1    |1.0           |0.0               |\n",
        "|AE     |1984|4482959  |5    |5.0           |0.0               |\n",
        "|AE     |1985|4554981  |14   |14.0          |0.0               |\n",
        "|AE     |1987|4663181  |3    |3.0           |0.0               |\n",
        "|AE     |1989|4805221  |7    |5.0           |2.0               |\n",
        "|AE     |1990|4909321  |2    |2.0           |0.0               |\n",
        "|AE     |1991|5004552  |3    |2.0           |1.0               |\n",
        "|AE     |1992|5104556  |4    |4.0           |0.0               |\n",
        "|AE     |1993|5181569  |8    |8.0           |0.0               |\n",
        "|AE     |1996|5580125  |1    |1.0           |0.0               |\n",
        "|AG     |1978|4126850  |6    |6.0           |0.0               |\n",
        "|AG     |1979|4172981  |14   |14.0          |0.0               |\n",
        "|AG     |1991|5013035  |8    |8.0           |0.0               |\n",
        "|AG     |1994|5345071  |19   |19.0          |0.0               |\n",
        "|AG     |1995|5457307  |11   |6.0           |5.0               |\n",
        "|AG     |1996|5525786  |2    |1.5           |0.5               |\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Parquet files created in Exercise 12.1\n",
        "citations_df = spark.read.parquet(\"/tmp/data/citations_count.parquet\")\n",
        "patents_df = spark.read.parquet(\"/tmp/data/apat_selected.parquet\")\n",
        "\n",
        "# Create temporary views for the DataFrames\n",
        "citations_df.createOrReplaceTempView(\"citations\")\n",
        "patents_df.createOrReplaceTempView(\"patents\")\n",
        "\n",
        "# SQL Query 1: Join patents and citations\n",
        "joined_query = \"\"\"\n",
        "    SELECT\n",
        "        p.country,\n",
        "        p.Year,\n",
        "        p.PatentNum,\n",
        "        c.ncitations\n",
        "    FROM patents p\n",
        "    INNER JOIN citations c\n",
        "    ON p.PatentNum = c.Cited\n",
        "\"\"\"\n",
        "joined_df = spark.sql(joined_query)\n",
        "joined_df.createOrReplaceTempView(\"joined\")\n",
        "\n",
        "# SQL Query 2: Compute max and average citations\n",
        "aggregated_query = \"\"\"\n",
        "    SELECT\n",
        "        country,\n",
        "        Year,\n",
        "        MAX(ncitations) AS max,\n",
        "        AVG(ncitations) AS average\n",
        "    FROM joined\n",
        "    GROUP BY country, Year\n",
        "\"\"\"\n",
        "aggregated_df = spark.sql(aggregated_query)\n",
        "aggregated_df.createOrReplaceTempView(\"aggregated\")\n",
        "\n",
        "# SQL Query 3: Find patents with the maximum number of citations\n",
        "result_query = \"\"\"\n",
        "    SELECT DISTINCT\n",
        "        agg.country AS Country,\n",
        "        agg.Year AS Year,\n",
        "        j.PatentNum AS PatentNum,\n",
        "        agg.max AS max,\n",
        "        agg.average AS average,\n",
        "        (agg.max - agg.average) AS diff\n",
        "    FROM aggregated agg\n",
        "    INNER JOIN joined j\n",
        "    ON agg.country = j.country\n",
        "       AND agg.Year = j.Year\n",
        "       AND agg.max = j.ncitations\n",
        "    ORDER BY Country, Year\n",
        "\"\"\"\n",
        "result_df = spark.sql(result_query)\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "print(\"Final DataFrame:\")\n",
        "result_df.show(10, truncate=False)\n",
        "\n",
        "# Save the resulting DataFrame as a single CSV file with a header and no compression\n",
        "output_path = \"/tmp/data/patents_max_avg_diff_sql.csv\"\n",
        "result_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Verify the saved CSV file\n",
        "print(\"Saved CSV File:\")\n",
        "!ls -lh {output_path}\n"
      ],
      "metadata": {
        "id": "Mt8r20goZ6sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektPyLWzaImT"
      },
      "source": [
        "## Exercise 12.5\n",
        "\n",
        "From the Parquet file with the (PatentNum,Country,Year) information from Exercise 12.1, create a DataFrame that shows the number of patents associated to each country per decade (understanding as a *decade* the years from 0 to 9; e.g. from 1970 to 1979). In addition, the DataFrame must show the increase or decrease of the number of patents per country and decade with respect to the previous decade. The resulting DataFrame must look like this:\n",
        "\n",
        "|Country|Decade|PatentsNum|Diff|\n",
        "|-------|------|----------|----|\n",
        "|AD     |1980  |1         |0   |\n",
        "|AD     |1990  |5         |4   |\n",
        "|AE     |1980  |7         |0   |\n",
        "|AE     |1990  |11        |4   |\n",
        "|AG     |1970  |2         |0   |\n",
        "|AG     |1990  |7         |5   |\n",
        "|AI     |1990  |1         |0   |\n",
        "|AL     |1990  |1         |0   |\n",
        "|AM     |1990  |2         |0   |\n",
        "|AN     |1970  |1         |0   |\n",
        "|AN     |1980  |2         |1   |\n",
        "|AN     |1990  |5         |3   |\n",
        "|AR     |1960  |135       |0   |\n",
        "|AR     |1970  |239       |104 |\n",
        "|AR     |1980  |184       |-55 |\n",
        "|AR     |1990  |292       |108 |\n",
        "\n",
        "**Requirements**\n",
        "\n",
        "- The DataFrame must be sorted by country code and year.\n",
        "- Do **NOT** replace the country code by its whole name.\n",
        "- The output must be saved as a single CSV file, with a header and without any compression."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Load the Parquet file with PatentNum, Country, and Year information\n",
        "patents_df = spark.read.parquet(\"/tmp/data/apat_selected.parquet\")\n",
        "\n",
        "# Compute the decade for each year\n",
        "patents_with_decade_df = patents_df.withColumn(\n",
        "    \"Decade\",\n",
        "    (F.col(\"Year\") / 10).cast(\"int\") * 10  # Compute the decade by truncating the year to its decade\n",
        ")\n",
        "\n",
        "# Group by country and decade, and count the number of patents\n",
        "grouped_df = patents_with_decade_df.groupBy(\"country\", \"Decade\").agg(\n",
        "    F.count(\"PatentNum\").alias(\"PatentsNum\")  # Count the number of patents\n",
        ")\n",
        "\n",
        "# Use a window function to compute the difference with the previous decade\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Define a window partitioned by country and ordered by decade\n",
        "window_spec = Window.partitionBy(\"country\").orderBy(\"Decade\")\n",
        "\n",
        "# Compute the difference with the previous decade\n",
        "result_df = grouped_df.withColumn(\n",
        "    \"Diff\",\n",
        "    F.col(\"PatentsNum\") - F.lag(\"PatentsNum\", 1).over(window_spec)  # Compute difference with the previous decade\n",
        ").fillna(0, subset=[\"Diff\"])  # Fill null differences (first decade) with 0\n",
        "\n",
        "final_df = result_df.orderBy(\"country\", \"Decade\")\n",
        "\n",
        "print(\"Final DataFrame:\")\n",
        "final_df.show(10, truncate=False)\n",
        "\n",
        "# Save the resulting DataFrame as a single CSV file with a header and no compression\n",
        "output_path = \"/tmp/data/patents_per_decade.csv\"\n",
        "final_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "print(\"Saved CSV File:\")\n",
        "!ls -lh {output_path}\n"
      ],
      "metadata": {
        "id": "MsnXojUSeRyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_non_empty = df.filter(F.col(\"line\").isNotNull() & (F.col(\"line\") != \"\"))\n",
        "df_with_word_count = df_non_empty.withColumn(\"word_count\", F.size(F.split(F.col(\"line\"), \" \")))\n",
        "df_with_word_count.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "CS5fbCmKcPlo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}