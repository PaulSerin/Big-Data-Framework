{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HVKsdEbEVi3T"
      ],
      "authorship_tag": "ABX9TyMcBTmBtkGjLHrmMJ3jwgOn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulSerin/Big-Data-Framework/blob/main/Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercises**"
      ],
      "metadata": {
        "id": "rF2yIjKnS_5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Spark environment setup*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HVKsdEbEVi3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtjPnwPPS_PC",
        "outputId": "fb34654b-e833-47f6-ba85-b78b2da28b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2024-11-26 08:22:45--  http://apache.osuosl.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 64.50.236.52, 64.50.233.100, 140.211.166.134, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|64.50.236.52|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400864419 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.3-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.3-bin-had 100%[===================>] 382.29M  19.4MB/s    in 27s     \n",
            "\n",
            "2024-11-26 08:23:12 (14.1 MB/s) - ‘spark-3.5.3-bin-hadoop3.tgz’ saved [400864419/400864419]\n",
            "\n",
            "spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Big Data Framework/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8maMDLUJThlo",
        "outputId": "03cdb9d7-9715-4e41-cec6-c02a68118087"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/Big Data Framework/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlieYwSpTjOX",
        "outputId": "26471fe9-de5b-42bb-9d5e-fd82852b48e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "PySpark version 3.5.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "EB18ZsqVTq_G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Wof7pCTtEm",
        "outputId": "9bde0a8b-242c-4de8-da66-0cdc47f8d7ba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ULPx4Y1LiR"
      },
      "source": [
        "## **Exercise 3.1: Word count**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of words per line in the $DRIVE_DATA/quijote.txt file.\n",
        "\n",
        "Repeat the exercise but this time counting the number of words in the whole file."
      ],
      "metadata": {
        "id": "sPAq-ILTkTgV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "c7Q_ljrX5RtE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "# so that we can use the F.split() function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Méthode 1 (Dataframe)"
      ],
      "metadata": {
        "id": "fEjuV31kaqpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Charger le fichier texte dans un DataFrame\n",
        "file_path = os.environ[\"DRIVE_DATA\"] + \"quijote.txt\"\n",
        "df = spark.read.text(file_path).withColumnRenamed(\"value\", \"line\")\n",
        "\n",
        "# Étape 2 : Compter les mots par ligne\n",
        "df_with_word_count = df.withColumn(\"word_count\", F.size(F.split(F.col(\"line\"), \" \")))\n",
        "df_with_word_count.show(10, truncate=False)\n",
        "\n",
        "# Étape 3 : Compter les mots dans tout le fichier\n",
        "total_word_count = df_with_word_count.agg(F.sum(\"word_count\").alias(\"total_word_count\"))\n",
        "total_word_count.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrHIf1flWkM2",
        "outputId": "d0bd05f7-cb7a-4d8e-9b42-64f42d94acd9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------+----------+\n",
            "|line                                                                       |word_count|\n",
            "+---------------------------------------------------------------------------+----------+\n",
            "|The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra|12        |\n",
            "|                                                                           |1         |\n",
            "|This eBook is for the use of anyone anywhere at no cost and with           |14        |\n",
            "|almost no restrictions whatsoever.  You may copy it, give it away or       |13        |\n",
            "|re-use it under the terms of the Project Gutenberg License included        |11        |\n",
            "|with this eBook or online at www.gutenberg.net                             |7         |\n",
            "|                                                                           |1         |\n",
            "|                                                                           |1         |\n",
            "|Title: Don Quijote                                                         |3         |\n",
            "|                                                                           |1         |\n",
            "+---------------------------------------------------------------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------------+\n",
            "|total_word_count|\n",
            "+----------------+\n",
            "|          393764|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : this method counts 1 word for empty lines.\n",
        "\n",
        "We will filter thoses empty lines before"
      ],
      "metadata": {
        "id": "HW3Se-lWbyvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_non_empty = df.filter(F.col(\"line\").isNotNull() & (F.col(\"line\") != \"\"))\n",
        "df_with_word_count = df_non_empty.withColumn(\"word_count\", F.size(F.split(F.col(\"line\"), \" \")))\n",
        "df_with_word_count.show(10, truncate=False)"
      ],
      "metadata": {
        "id": "CS5fbCmKcPlo",
        "outputId": "926c41a0-2e60-4b94-8ee8-36f66f58dff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------+----------+\n",
            "|line                                                                       |word_count|\n",
            "+---------------------------------------------------------------------------+----------+\n",
            "|The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra|12        |\n",
            "|This eBook is for the use of anyone anywhere at no cost and with           |14        |\n",
            "|almost no restrictions whatsoever.  You may copy it, give it away or       |13        |\n",
            "|re-use it under the terms of the Project Gutenberg License included        |11        |\n",
            "|with this eBook or online at www.gutenberg.net                             |7         |\n",
            "|Title: Don Quijote                                                         |3         |\n",
            "|Author: Miguel de Cervantes Saavedra                                       |5         |\n",
            "|Posting Date: April 27, 2010 [EBook #2000]                                 |7         |\n",
            "|Release Date: December, 1999                                               |4         |\n",
            "|Language: Spanish                                                          |2         |\n",
            "+---------------------------------------------------------------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Méthode 2 (RDD)"
      ],
      "metadata": {
        "id": "_rvzdn8_amiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the text file as an RDD\n",
        "rdd = sc.textFile(file_path)\n",
        "\n",
        "# Step 0: Filter out empty lines\n",
        "# Strip removes leading and trailing whitespaces, and we ensure the line is not empty\n",
        "rdd_non_empty = rdd.filter(lambda line: line.strip() != \"\")\n",
        "\n",
        "# Step 1: Count the number of words per line (excluding empty lines)\n",
        "# Split each non-empty line into words and count the number of words\n",
        "word_count_per_line = rdd_non_empty.map(lambda line: len(line.split(\" \")))\n",
        "print(\"Word count per line (excluding empty lines):\")\n",
        "print(word_count_per_line.take(10))  # Display the first 10 results\n",
        "\n",
        "# Step 2: Count the total number of words in the file (excluding empty lines)\n",
        "# Split all non-empty lines into words, flatten the resulting lists, and count the total\n",
        "total_word_count_rdd = rdd_non_empty.flatMap(lambda line: line.split(\" \")).count()\n",
        "print(\"Total word count in the file (excluding empty lines):\", total_word_count_rdd)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph_2Rl7LX7o4",
        "outputId": "ef324e8e-5ae5-4b69-dadb-1235fa6c4621"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count per line (excluding empty lines):\n",
            "[12, 14, 13, 11, 7, 3, 5, 7, 4, 2]\n",
            "Total word count in the file (excluding empty lines): 387834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Méthode 3 (function SQL)"
      ],
      "metadata": {
        "id": "44Mg-Kd3auUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary view\n",
        "df.createOrReplaceTempView(\"lines\")\n",
        "\n",
        "# Step 1: Count the number of words per line (excluding empty lines)\n",
        "# Filter out rows where the line is NULL or empty\n",
        "word_count_query = \"\"\"\n",
        "SELECT line,\n",
        "       SIZE(SPLIT(line, ' ')) AS word_count\n",
        "FROM lines\n",
        "WHERE line IS NOT NULL AND line != ''\n",
        "\"\"\"\n",
        "df_word_count = spark.sql(word_count_query)\n",
        "df_word_count.show(10)  # Display the first 10 results\n",
        "\n",
        "# Step 2: Count the total number of words in the file (excluding empty lines)\n",
        "# Again, exclude rows where the line is NULL or empty\n",
        "total_word_count_query = \"\"\"\n",
        "SELECT SUM(SIZE(SPLIT(line, ' '))) AS total_word_count\n",
        "FROM lines\n",
        "WHERE line IS NOT NULL AND line != ''\n",
        "\"\"\"\n",
        "df_total_word_count = spark.sql(total_word_count_query)\n",
        "df_total_word_count.show()  # Display the total word count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ju6PfeAavez",
        "outputId": "d5809e5f-9d8c-4dfa-ff2c-76fa61f7c629"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|                line|word_count|\n",
            "+--------------------+----------+\n",
            "|The Project Guten...|        12|\n",
            "|This eBook is for...|        14|\n",
            "|almost no restric...|        13|\n",
            "|re-use it under t...|        11|\n",
            "|with this eBook o...|         7|\n",
            "|  Title: Don Quijote|         3|\n",
            "|Author: Miguel de...|         5|\n",
            "|Posting Date: Apr...|         7|\n",
            "|Release Date: Dec...|         4|\n",
            "|   Language: Spanish|         2|\n",
            "+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------------+\n",
            "|total_word_count|\n",
            "+----------------+\n",
            "|          387834|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 4.1: Pi Estimation**"
      ],
      "metadata": {
        "id": "1YH9-PfYT1Xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Monte Carlo method, estimate the value of Pi. Use the random() method from the random class."
      ],
      "metadata": {
        "id": "Y410Vk6LT_-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YdVciAgNTu40"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With RDD :"
      ],
      "metadata": {
        "id": "V_MnlIAx3Epc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Pi Estimation\").getOrCreate()\n",
        "\n",
        "# Step 2: Number of points to simulate\n",
        "NUM_POINTS = 1000000\n",
        "\n",
        "# Step 3: Create an RDD with NUM_POINTS random samples\n",
        "rdd = spark.sparkContext.parallelize(range(NUM_POINTS))\n",
        "\n",
        "# Step 4: Function to determine if a point is inside the unit circle\n",
        "def is_inside_unit_circle(_):\n",
        "    x = random.random()\n",
        "    y = random.random()\n",
        "    return x**2 + y**2 <= 1\n",
        "\n",
        "# Step 5: Map points to 1 if inside circle, 0 otherwise\n",
        "points_inside_circle = rdd.map(is_inside_unit_circle).filter(lambda inside: inside == True).count()\n",
        "\n",
        "# Step 6: Calculate Pi\n",
        "pi_estimate = 4 * (points_inside_circle / NUM_POINTS)\n",
        "\n",
        "# Step 7: Output the result\n",
        "print(f\"Estimated value of Pi using {NUM_POINTS} points: {pi_estimate}\")\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5TMjlA5zDL4",
        "outputId": "122bade8-7f8f-40f6-b26f-3a35af99bab0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated value of Pi using 1000000 points: 3.143184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With a Dataframe :"
      ],
      "metadata": {
        "id": "3bUa_BvR2_a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand, pow, col\n",
        "\n",
        "# Step 0: Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"Pi Estimation\").getOrCreate()\n",
        "\n",
        "# Step 1: Number of points to simulate\n",
        "NUM_POINTS = 1000000  # Increase for more accuracy\n",
        "\n",
        "# Step 2: Create a DataFrame with random x and y points\n",
        "df_points = (\n",
        "    spark.range(NUM_POINTS)\n",
        "    .select(\n",
        "        rand(seed=0).alias(\"x\"),\n",
        "        rand(seed=1).alias(\"y\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 3: Add a column to determine if the point is inside the unit circle\n",
        "df_inside_circle = df_points.withColumn(\n",
        "    \"inside_circle\", (pow(col(\"x\"), 2) + pow(col(\"y\"), 2) <= 1).cast(\"int\")\n",
        ")\n",
        "\n",
        "# Step 4: Count points inside the circle\n",
        "points_inside_circle = df_inside_circle.agg({\"inside_circle\": \"sum\"}).collect()[0][0]\n",
        "\n",
        "# Step 5: Calculate Pi\n",
        "pi_estimate = 4 * (points_inside_circle / NUM_POINTS)\n",
        "\n",
        "# Step 6: Output the result\n",
        "print(f\"Estimated value of Pi using {NUM_POINTS} points: {pi_estimate}\")\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nnpiWGIz_j8",
        "outputId": "a1e8b925-0f34-42d4-a771-1788f188df01"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated value of Pi using 1000000 points: 3.137908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vPYCnS8ERmg"
      },
      "source": [
        "## Exercise 4.2: Inspect a log file\n",
        "\n",
        "Upload the file /var/log/syslog from your computer to this notebook. Then, select only the \"bad lines\": WARNING and ERROR messages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "RTo7oJej43Zv",
        "outputId": "52a97468-f46a-4b8f-d087-f805edf2680e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c20b188e-317f-4f17-9f7e-0c0c8052fd6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c20b188e-317f-4f17-9f7e-0c0c8052fd6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving syslog to syslog (1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Demr8raN4yIx",
        "outputId": "8bda3ed0-d260-4e06-c2e6-41152fc7531c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " gdrive   sample_data   spark   spark-3.5.3-bin-hadoop3   syslog  'syslog (1)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VR-Men-G5ZdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06155586-379e-4b4d-bf79-23b27a6dd267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad lines (WARNING and ERROR messages):\n",
            "Nov 25 17:22:07 paul-serin ovpn-cytech.students[1358]: WARNING: file '/data/CYTECH-VPN-CLE-PERSONNELLE/client.p12' is group or others accessible\n",
            "Nov 25 17:22:07 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:22:07 paul-serin gnome-session[1456]: gnome-session-binary[1456]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 25 17:22:07 paul-serin gnome-session-binary[1456]: WARNING: Falling back to non-systemd startup procedure due to error: GDBus.Error:org.freedesktop.DBus.Error.Spawn.ChildExited: Process org.freedesktop.systemd1 exited with status 1\n",
            "Nov 25 17:22:15 paul-serin gnome-shell[1481]: JS WARNING: [resource:///org/gnome/shell/ui/layout.js 24]: reference to undefined property \"MetaWindowXwayland\"\n",
            "Nov 25 17:22:21 paul-serin gnome-session[2098]: gnome-session-binary[2098]: WARNING: Could not parse desktop file nautilus-autostart.desktop or it references a not found TryExec binary\n",
            "Nov 25 17:22:21 paul-serin gnome-session-binary[2098]: WARNING: Could not parse desktop file nautilus-autostart.desktop or it references a not found TryExec binary\n",
            "Nov 25 17:22:22 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:22:29 paul-serin gnome-shell[2515]: [2509:2509:1125/172229.203746:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.ScreenSaver.GetActive: object_path= /org/freedesktop/ScreenSaver: org.freedesktop.DBus.Error.NotSupported: This method is not implemented\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2537:1125/172232.122190:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2537:1125/172232.122548:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2537:1125/172232.122613:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2537:1125/172232.122660:ERROR:registration_request.cc(274)] Registration URL fetching failed.\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2509:1125/172232.852309:ERROR:interface_endpoint_client.cc(725)] Message 1 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 25 17:22:32 paul-serin gnome-shell[2515]: [2509:2509:1125/172232.852532:ERROR:interface_endpoint_client.cc(725)] Message 1 rejected by interface blink.mojom.WidgetHost\n",
            "Nov 25 17:22:37 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:22:47 paul-serin gnome-shell[2515]: [2509:2509:1125/172247.136366:ERROR:account_info_fetcher.cc(62)] OnGetTokenFailure: Invalid credentials (credentials rejected by server).\n",
            "Nov 25 17:22:52 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:22:52 paul-serin gnome-shell[2515]: [2509:2537:1125/172252.845564:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 25 17:23:38 paul-serin gnome-shell[2515]: [2509:2537:1125/172338.434705:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 25 17:24:09 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:25:09 paul-serin gnome-shell[2515]: [2509:2537:1125/172509.535766:ERROR:registration_request.cc(291)] Registration response error message: DEPRECATED_ENDPOINT\n",
            "Nov 25 17:25:21 paul-serin discord_discord.desktop[5622]: [5622:1125/172521.737196:ERROR:zygote_host_impl_linux.cc(273)] Failed to adjust OOM score of renderer with pid 5783: Permission denied (13)\n",
            "Nov 25 17:25:22 paul-serin discord_discord.desktop[5810]: [5810:1125/172522.612847:ERROR:ffmpeg_common.cc(965)] Unsupported pixel format: -1\n",
            "Nov 25 17:25:24 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:25:33 paul-serin discord_discord.desktop[5622]: [5622:1125/172533.292692:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.login1.Manager.Inhibit: object_path= /org/freedesktop/login1: org.freedesktop.DBus.Error.AccessDenied: An AppArmor policy prevents this sender from sending this message to this recipient; type=\"method_call\", sender=\":1.170\" (uid=1000 pid=5622 comm=\"/snap/discord/216/usr/share/discord/Discord --use-\" label=\"snap.discord.discord (enforce)\") interface=\"org.freedesktop.login1.Manager\" member=\"Inhibit\" error name=\"(unset)\" requested_reply=\"0\" destination=\"org.freedesktop.login1\" (uid=0 pid=962 comm=\"/lib/systemd/systemd-logind \" label=\"unconfined\")\n",
            "Nov 25 17:25:36 paul-serin discord_discord.desktop[5622]: [5622:1125/172536.571434:ERROR:object_proxy.cc(576)] Failed to call method: org.freedesktop.Secret.Service.ReadAlias: object_path= /org/freedesktop/secrets: org.freedesktop.DBus.Error.AccessDenied: An AppArmor policy prevents this sender from sending this message to this recipient; type=\"method_call\", sender=\":1.123\" (uid=1000 pid=5622 comm=\"/snap/discord/216/usr/share/discord/Discord --use-\" label=\"snap.discord.discord (enforce)\") interface=\"org.freedesktop.Secret.Service\" member=\"ReadAlias\" error name=\"(unset)\" requested_reply=\"0\" destination=\"org.freedesktop.secrets\" (uid=1000 pid=1967 comm=\"/usr/bin/gnome-keyring-daemon --daemonize --login \" label=\"unconfined\")\n",
            "Nov 25 17:25:39 paul-serin discord_discord.desktop[5886]: [5886:1125/172539.449943:ERROR:ffmpeg_common.cc(965)] Unsupported pixel format: -1\n",
            "Nov 25 17:26:33 paul-serin discord_discord.desktop[5622]: [5622:1125/172633.190294:ERROR:atom_cache.cc(230)] Add chromium/from-privileged to kAtomsToCache\n",
            "Nov 25 17:26:39 paul-serin ovpn-cytech.students[1358]: WARNING: Your certificate has expired!\n",
            "Nov 25 17:26:50 paul-serin discord_discord.desktop[5622]: 17:26:50.279 › [UploaderBase.tsx] _handleError: undefined ({\"type\":\"ERROR_SOURCE_UNKNOWN\",\"msg\":\"Error: File upload9 failed to upload\"}) for Uploader11\n",
            "Nov 25 17:26:50 paul-serin discord_discord.desktop[5622]: 17:26:50.673 › [UploaderBase.tsx] _handleError: undefined ({\"type\":\"ERROR_SOURCE_UNKNOWN\",\"msg\":\"Error: Unhandled error. (undefined)\"}) for Uploader11\n",
            "Nov 25 17:27:01 paul-serin gnome-shell[2515]: [2509:2533:1125/172701.160397:ERROR:cert_verify_proc_builtin.cc(1063)] CertVerifyProcBuiltin for arel.cy-tech.fr failed:\n",
            "Nov 25 17:27:01 paul-serin gnome-shell[2515]: ERROR: Time is after notAfter\n",
            "Nov 25 17:27:01 paul-serin gnome-shell[2515]: [2571:2578:1125/172701.160792:ERROR:ssl_client_socket_impl.cc(882)] handshake failed; returned -1, SSL error code 1, net_error -201\n",
            "Nov 25 17:27:31 paul-serin discord_discord.desktop[5622]: 17:27:31.012 › [UploaderBase.tsx] _handleError: undefined ({\"type\":\"ERROR_SOURCE_UNKNOWN\",\"msg\":\"Error: File upload12 failed to upload\"}) for Uploader14\n",
            "Nov 25 17:27:31 paul-serin discord_discord.desktop[5622]: 17:27:31.352 › [UploaderBase.tsx] _handleError: undefined ({\"type\":\"ERROR_SOURCE_UNKNOWN\",\"msg\":\"Error: Unhandled error. (undefined)\"}) for Uploader14\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Assuming a SparkSession is already active\n",
        "# If not, uncomment the line below to initialize it\n",
        "spark = SparkSession.builder.appName(\"Inspect Log File\").getOrCreate()\n",
        "\n",
        "# Step 2: Load the syslog file\n",
        "log_file_path = \"/content/syslog\"  # Replace with the uploaded file path\n",
        "rdd = spark.sparkContext.textFile(log_file_path)\n",
        "\n",
        "# Step 3: Filter lines containing \"WARNING\" or \"ERROR\"\n",
        "bad_lines = rdd.filter(lambda line: \"WARNING\" in line or \"ERROR\" in line)\n",
        "\n",
        "# Step 4: Collect and display the bad lines\n",
        "bad_lines_collected = bad_lines.collect()\n",
        "print(\"Bad lines (WARNING and ERROR messages):\")\n",
        "for line in bad_lines_collected:\n",
        "    print(line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataframe approach"
      ],
      "metadata": {
        "id": "dRBstjBoKYNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, lit\n",
        "\n",
        "# Step 1: Load the syslog file into a DataFrame\n",
        "df_logs = spark.read.text(log_file_path).withColumnRenamed(\"value\", \"line\")\n",
        "\n",
        "# Step 2: Add a column to classify lines as \"bad\" or \"good\"\n",
        "# A \"bad\" line contains \"WARNING\" or \"ERROR\", otherwise it is \"good\"\n",
        "df_logs = df_logs.withColumn(\n",
        "    \"is_bad\",\n",
        "    when((col(\"line\").contains(\"WARNING\")) | (col(\"line\").contains(\"ERROR\")), lit(1)).otherwise(lit(0))\n",
        ")\n",
        "\n",
        "# Step 3: Count the number of bad and good lines\n",
        "bad_lines_count = df_logs.filter(col(\"is_bad\") == 1).count()\n",
        "good_lines_count = df_logs.filter(col(\"is_bad\") == 0).count()\n",
        "\n",
        "# Step 4: Show results\n",
        "print(f\"Number of bad lines (WARNING or ERROR): {bad_lines_count}\")\n",
        "print(f\"Number of good lines: {good_lines_count}\")\n"
      ],
      "metadata": {
        "id": "S7OoWtMlKTls",
        "outputId": "b1970200-430c-4477-c513-3e33a8c03c72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bad lines (WARNING or ERROR): 37\n",
            "Number of good lines: 10578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ708zvl627q"
      },
      "source": [
        "## **Exercise 5.1: Word count**\n",
        "\n",
        "**Using RDDs**, count the number of lines in the `$DRIVE_DATA/quijote.txt` file. Then, count the number of words in the file. Finally, count the number of *different* words in the file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Slr4nXhp_knl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229c8526-b737-427c-f7fd-e313e5a0d9b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 31931\n",
            "Number of words: 387834\n",
            "Number of different words: 40060\n"
          ]
        }
      ],
      "source": [
        "file_path = f\"{os.environ['DRIVE_DATA']}quijote.txt\"\n",
        "\n",
        "rdd = spark.sparkContext.textFile(file_path)\n",
        "\n",
        "rdd = rdd.filter(lambda line: line.strip() != \"\") # We remove empty lines\n",
        "\n",
        "num_lines = rdd.count()\n",
        "\n",
        "num_words = rdd.flatMap(lambda line: line.split(\" \")).count()\n",
        "\n",
        "num_distinct_words = rdd.flatMap(lambda line: line.split(\" \")).distinct().count()\n",
        "\n",
        "print(f\"Number of lines: {num_lines}\")\n",
        "print(f\"Number of words: {num_words}\")\n",
        "print(f\"Number of different words: {num_distinct_words}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjtVkYOKxTo7"
      },
      "source": [
        "## **Exercise 5.2: Count people by age**\n",
        "\n",
        "Using RDDs, create a barplot showing of number of people (y-axis) per age (x-axis) using the information in the $DRIVE_DATA/people.txt file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JgE0XavBBLB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "f56a9275-91bc-4c14-ab90-5b5865e53c26"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "file_path = f\"{os.environ['DRIVE_DATA']}people.txt\"\n",
        "\n",
        "rdd = spark.sparkContext.textFile(file_path)\n",
        "\n",
        "# Extract age and count occurrences\n",
        "age_counts = rdd.map(lambda line: line.split(\"\\t\")[1])\n",
        "age_counts = age_counts.map(lambda age: (age, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Collect the results\n",
        "age_counts_data = age_counts.collect()\n",
        "\n",
        "# Sort for plotting\n",
        "ages, counts = zip(*sorted(age_counts_data, key=lambda x: int(x[0])))\n",
        "\n",
        "# Create the bar plot\n",
        "plt.bar(ages, counts)\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Number of People\")\n",
        "plt.title(\"Number of People per Age\")\n",
        "#plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xddDZ2zCxTor"
      },
      "source": [
        "### **Exercise 5.3: Obtain the number of received citations**\n",
        "\n",
        "Using RDDs, write a PySpark program that obtains, from the cite75_99.txt file, the number of citations received by each patent.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = f\"{os.environ['DRIVE_DATA']}cite75_99.txt.tar.bz2\"\n",
        "\n",
        "# Load the file into an RDD\n",
        "rdd = spark.sparkContext.textFile(file_path)\n",
        "\n",
        "rdd.take(10)\n",
        "\n",
        "# Extract cited patents and count citations\n",
        "citations = rdd.map(lambda line: line.split(\",\")[1])  # Assuming '->' separates the citing and cited patents\n",
        "citations_count = citations.map(lambda patent: (patent, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Collect and display results\n",
        "citations_data = citations_count.collect()\n",
        "print(\"Number of citations received by each patent:\")\n",
        "for patent, count in sorted(citations_data, key=lambda x: -x[1]):\n",
        "    print(f\"Patent {patent}: {count} citations\")\n"
      ],
      "metadata": {
        "id": "QvMWJc0Sj_6l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2131a06-b5cc-472c-92f4-a64a46ef6ae3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 8.0 failed 1 times, most recent failure: Lost task 2.0 in stage 8.0 (TID 9) (0a93410557e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-31-06c2011bb282>\", line 9, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-31-06c2011bb282>\", line 9, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-06c2011bb282>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Collect and display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcitations_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcitations_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of citations received by each patent:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitations_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 8.0 failed 1 times, most recent failure: Lost task 2.0 in stage 8.0 (TID 9) (0a93410557e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-31-06c2011bb282>\", line 9, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 5434, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/content/spark/python/pyspark/rdd.py\", line 840, in func\n    return f(iterator)\n  File \"/content/spark/python/pyspark/rdd.py\", line 3983, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"/content/spark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-31-06c2011bb282>\", line 9, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
          ]
        }
      ]
    }
  ]
}