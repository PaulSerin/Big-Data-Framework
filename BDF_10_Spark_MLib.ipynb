{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulSerin/Big-Data-Framework/blob/main/BDF_10_Spark_MLib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf",
        "outputId": "b8ff8e5b-7e82-4493-d1af-25c0a8835eff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [Connected to cloud.r-\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [2 InRelease 28.7 kB/128 kB 22%] [Waiting for headers] [Connected to cloud.r-project.org (108.139\r                                                                                                    \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,734 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,512 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,172 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,619 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,223 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,506 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,452 kB]\n",
            "Fetched 20.6 MB in 5s (4,189 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2024-11-26 14:00:26--  http://apache.osuosl.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 64.50.233.100, 64.50.236.52, 140.211.166.134, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|64.50.233.100|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400864419 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.3-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.3-bin-had 100%[===================>] 382.29M  62.3MB/s    in 41s     \n",
            "\n",
            "2024-11-26 14:01:07 (9.30 MB/s) - ‘spark-3.5.3-bin-hadoop3.tgz’ saved [400864419/400864419]\n",
            "\n",
            "spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx",
        "outputId": "9d586af2-cca4-40f5-971e-10f194b44240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Big Data Framework/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/spark': No such file or directory\n",
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/Big Data Framework/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN",
        "outputId": "a0bc8ddf-1b74-48aa-c87a-ddcac744d246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "PySpark version 3.5.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT",
        "outputId": "c5e17a9d-688c-4ad3-98c3-56ae2d562ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 10 - Spark MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vCoMnv84_Hc"
      },
      "source": [
        "Library of ML parallel algorithms for massive data\n",
        "\n",
        "-   Machine learning classic algorithms: classification, regression, clustering, collaborative filtering\n",
        "-   Other algorithms: feature extraction, transformation, dimensionality reduction, and selection\n",
        "-   Tools to build, evaluate and adjust ML pipelines\n",
        "-   Other tools: linear algebra, statistics, data processing, etc.\n",
        "\n",
        "\n",
        "Two packages:\n",
        "\n",
        "-   **spark.mllib:** Original RDD-based API\n",
        "-   **spark.ml:** High-level API, based on DataFrames\n",
        "\n",
        "Documentation and APIS:\n",
        "\n",
        "- ML\n",
        "    - Guide: http://spark.apache.org/docs/latest/ml-guide.html\n",
        "    - API Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html\n",
        "    - API Scala: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.package\n",
        "- MLlib\n",
        "    - Guide: http://spark.apache.org/docs/latest/mllib-guide.html\n",
        "    - API Python: https://spark.apache.org/docs/latest/api/python/reference/pyspark.mllib.html\n",
        "    - API Scala: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.package\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWMYgbMB5mLS"
      },
      "source": [
        "## Example\n",
        "\n",
        "Use the [KMeans](http://spark.apache.org/docs/latest/mllib-clustering.html#k-means) clustering algorithm to group data from vectors spread over two clusters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Me4Hx7jnbwBX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg-_EowF52_0",
        "outputId": "fb6e9da8-da09-42a4-c77e-743533b94470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.ml.clustering import KMeans, KMeansModel\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "#  Define an array of 4 sparse vectors, 3 elements each\n",
        "sparseData = [\n",
        "     Vectors.sparse(3, {1: 1.2}),\n",
        "     Vectors.sparse(3, {1: 1.1}),\n",
        "     Vectors.sparse(3, {0: 0.9, 2: 1.0}),\n",
        "     Vectors.sparse(3, {0: 1.0, 2: 1.1})\n",
        " ]\n",
        "\n",
        "for i in range(4):\n",
        "    print(sparseData[i].toArray())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.  1.2 0. ]\n",
            "[0.  1.1 0. ]\n",
            "[0.9 0.  1. ]\n",
            "[1.  0.  1.1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FIwTWDw6Mzx",
        "outputId": "b32f9b19-4c9c-4cbd-d3da-f4f7fff3a110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Turn the array into a DataFrame\n",
        "dfSD = sc.parallelize([\n",
        "  (1, sparseData[0]),\n",
        "  (2, sparseData[1]),\n",
        "  (3, sparseData[2]),\n",
        "  (4, sparseData[3])\n",
        "]).toDF([\"row\", \"features\"])\n",
        "\n",
        "dfSD.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+\n",
            "|row|           features|\n",
            "+---+-------------------+\n",
            "|  1|      (3,[1],[1.2])|\n",
            "|  2|      (3,[1],[1.1])|\n",
            "|  3|(3,[0,2],[0.9,1.0])|\n",
            "|  4|(3,[0,2],[1.0,1.1])|\n",
            "+---+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9_4SXD7AqG"
      },
      "source": [
        "# Create a KMeans model without training, with 2 clusters\n",
        "# For more information, see https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#module-pyspark.ml.clustering\n",
        "kmeans = KMeans()\\\n",
        "    .setInitMode(\"k-means||\")\\\n",
        "    .setFeaturesCol(\"features\")\\\n",
        "    .setPredictionCol(\"prediction\")\\\n",
        "    .setK(2)\\\n",
        "    .setSeed(1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpfUDkqA9tju",
        "outputId": "23bf2924-0d3e-4289-d74a-a89c4717d7f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Adjust the model to the previous DataFrame and show the cluster centres\n",
        "kmModel = kmeans.fit(dfSD)\n",
        "print(\"Clusters centres: {0}\".format(\n",
        "    kmModel.clusterCenters()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clusters centres: [array([0.  , 1.15, 0.  ]), array([0.95, 0.  , 1.05])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyqXUOLk-B-X",
        "outputId": "a1841c60-17a2-42c1-bb55-321bb351dd49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Verify that the model clusters the data from the previous array\n",
        "kmModel.transform(dfSD).show()\n",
        "# Calculate the cost as the addition of the squared distance between the input points\n",
        "# and the centres of the corresponding clusters\n",
        "print(\"Cost = {0}\".format(\n",
        "    kmModel.summary.trainingCost))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+----------+\n",
            "|row|           features|prediction|\n",
            "+---+-------------------+----------+\n",
            "|  1|      (3,[1],[1.2])|         0|\n",
            "|  2|      (3,[1],[1.1])|         0|\n",
            "|  3|(3,[0,2],[0.9,1.0])|         1|\n",
            "|  4|(3,[0,2],[1.0,1.1])|         1|\n",
            "+---+-------------------+----------+\n",
            "\n",
            "Cost = 0.014999999999999236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG_qkJlC-ogG",
        "outputId": "3e76f711-1015-4c17-9c40-1ceb7693dad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Test the model with other points\n",
        "dfTest = sc.parallelize([\n",
        "  (1, Vectors.sparse(3, {0: 0.9, 1:1.0, 2: 1.0})),\n",
        "  (2, Vectors.sparse(3, {1: 1.5, 2: 0.3}))\n",
        "]).toDF([\"row\", \"features\"])\n",
        "\n",
        "kmModel.transform(dfTest).show(truncate=False)\n",
        "\n",
        "# Calculate the cost as the addition of the squared distance between the input points\n",
        "# and the centres of the corresponding clusters\n",
        "print(\"Cost = {0}\".format(\n",
        "    kmModel.summary.trainingCost))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------+----------+\n",
            "|row|features                 |prediction|\n",
            "+---+-------------------------+----------+\n",
            "|1  |(3,[0,1,2],[0.9,1.0,1.0])|1         |\n",
            "|2  |(3,[1,2],[1.5,0.3])      |0         |\n",
            "+---+-------------------------+----------+\n",
            "\n",
            "Cost = 0.014999999999999236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNwDiJtmBhtP"
      },
      "source": [
        "# Save the model in a local directory\n",
        "kmModel.save(\"/tmp/kmModel\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y23ahGDqBolO",
        "outputId": "9adf19c1-58fe-49e2-fbda-a5bfa59ab68f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Reload the model\n",
        "sameModel = KMeansModel.load(\"/tmp/kmModel\")\n",
        "\n",
        "sameModel.transform(dfTest).show(truncate=False)\n",
        "# Calculate the cost as the addition of the squared distance between the input points\n",
        "# and the centres of the corresponding clusters\n",
        "# print(\"Cost = {0}\".format(sameModel.summary.trainingCost))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------+----------+\n",
            "|row|features                 |prediction|\n",
            "+---+-------------------------+----------+\n",
            "|1  |(3,[0,1,2],[0.9,1.0,1.0])|1         |\n",
            "|2  |(3,[1,2],[1.5,0.3])      |0         |\n",
            "+---+-------------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnuGHaLXB3fL"
      },
      "source": [
        "!rm -rf /tmp/kmModel"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wk8w66tskzrh"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}