{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulSerin/Big-Data-Framework/blob/main/BDF_08_Running_Spark_on_a_cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL0HHBxQa1Hc"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcWXhOxia5yZ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 3.0.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsAfQ0CrgnWf",
        "outputId": "1b94be79-90cb-4acc-b380-c77c0537c530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-3.5.3\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget  http://apache.osuosl.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting for headers] [1 InRele\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [2 InRelease 0 B/3,626 B 0%] [Waiting for headers] [1\r0% [Waiting for headers] [Waiting for headers] [2 InRelease 3,626 B/3,626 B 100%] [Waiting for heade\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcon\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r                                                                                                    \rGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,172 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,506 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,619 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,223 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,452 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,512 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,734 kB]\n",
            "Fetched 20.6 MB in 7s (2,759 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "--2024-11-26 13:56:42--  http://apache.osuosl.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 140.211.166.134, 64.50.236.52, 64.50.233.100, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|140.211.166.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400864419 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.3-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.3-bin-had 100%[===================>] 382.29M  54.2MB/s    in 25s     \n",
            "\n",
            "2024-11-26 13:57:08 (15.2 MB/s) - ‘spark-3.5.3-bin-hadoop3.tgz’ saved [400864419/400864419]\n",
            "\n",
            "spark-3.5.3-bin-hadoop3.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urlhmQ_ra_ba"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiOoj3rUgnVx",
        "outputId": "68737fdb-1413-48af-a584-6a4e05fb2f43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Big Data Framework/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/spark': No such file or directory\n",
            "/content/spark/\n",
            "DRIVE_DATA=/content/gdrive/My Drive/Big Data Framework/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2URH7tCHbDqf"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8JD51WVauRN",
        "outputId": "87518d0b-255e-40a4-ff62-1744ff902793",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "PySpark version 3.5.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar81vEOHauP2"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBMAZitVauMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb36d22b-4f0c-4778-a397-116010774eb0"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajoV8LDbTCe"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 08 - Running Spark on a cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q4ebRam_m3Z"
      },
      "source": [
        "## Running a Spark program\n",
        "\n",
        "### Command `spark-submit`\n",
        "\n",
        "-   Submits a Spark program (an application) to a cluster\n",
        "-   More specifically, it launches the driver program and invokes the main() method specified by the user\n",
        "\n",
        "-   Examples:\n",
        "```sh\n",
        "$ bin/spark-submit --master yarn --deploy-mode cluster \\  \n",
        "     --py-files anotherlib.zip,anotherfile.py \\  \n",
        "     --num-executors 10 --executor-cores 2 \\  \n",
        "     my-script.py script_options\n",
        "```\n",
        "\n",
        "### `spark-submit` options\n",
        "\n",
        "-   `master`: cluster manager to use (options: `yarn`, `mesos://host:port`, `spark://host:port`, `local[n]`)\n",
        "\n",
        "-   `deploy-mode`: Two ways of deploying\n",
        "\n",
        "    -   `client`: runs the driver on the local node\n",
        "\n",
        "    -   `cluster`: runs the driver on a node of the cluster\n",
        "\n",
        "-   `class`: class to execute (Java or Scala)\n",
        "\n",
        "-   `name`: name of the application (it will be shown in the Spark web)\n",
        "\n",
        "-   `jars`: jar files to add to the classpath (Java o Scala)\n",
        "\n",
        "-   `py-files`: files to add to the PYTHONPATH (`.py`,`.zip`,`.egg`)\n",
        "\n",
        "-   `files`: data files for the applications\n",
        "\n",
        "-   `executor-memory`: total memory of each executor\n",
        "\n",
        "-   `driver-memory`: memory of the driver process\n",
        "\n",
        "For more options: `spark-submit --help`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr0Lfj_3_3Xy"
      },
      "source": [
        "!spark/bin/spark-submit --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MBC4c6_AgWb"
      },
      "source": [
        "![Spark-YARN: Client mode](https://i.pinimg.com/originals/e5/e6/a6/e5e6a6dbc4da4a2dbc1b54effd5995ee.jpg)\n",
        "\n",
        "\n",
        "![Spark-on-YARN: Cluster mode](https://i.pinimg.com/originals/db/16/e9/db16e98baed2a9b54c64e931e1f9b2b5.jpg)\n",
        "\n",
        "Source: [Spark-on-YARN: Empower Spark Applications on Hadoop Cluster](https://www.slideshare.net/Hadoop_Summit/sparkonyarn-empower-spark-applications-on-hadoop-cluster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0CiVeetECWE"
      },
      "source": [
        "## Configuration parameters\n",
        "\n",
        "Several parameters that can be adjusted in runtime\n",
        "\n",
        "-   In the script\n",
        "```python\n",
        "from pyspark import SparkConf,SparkContext\n",
        "conf = SparkConf()\n",
        "conf.set(\"spark.app.name\", \"My app\")\n",
        "conf.set(\"spark.master\", \"local[2]\") # Cluster manager local mode with 2 threads\n",
        "conf.set(\"spark.ui.port\", \"3600\")    # Port of the Spark web interface (by default: 4040)\n",
        "sc = SparkContext(conf=conf)\n",
        "```\n",
        "\n",
        "-   Using flags in the `spark-submit` command\n",
        "```sh\n",
        "$ bin/spark-submit --master local[2] --name \"My app\" \\  \n",
        "    --conf spark.ui.port=3600 my-script.py\n",
        "```    \n",
        "    \n",
        "-   Using a properties file\n",
        "```sh\n",
        "$ cat config.conf\n",
        "spark.master     local[2]\n",
        "spark.app.name   \"My app\"\n",
        "spark.ui.port 3600\n",
        "$ bin/spark-submit --properties-file config.conf my-script.py\n",
        "```\n",
        "\n",
        "More information: <http://spark.apache.org/docs/latest/configuration.html#spark-properties>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKwBitASEZrX"
      },
      "source": [
        "## Example: Python script execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK5ru0Z9JpJO"
      },
      "source": [
        "!cat \"$DRIVE_DATA\"/myscript.py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFuf5kOLCrnB",
        "outputId": "47eaad6d-e97c-4a64-98ed-07179914b62e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "# NOTE: It won't work in a notebook.\n",
        "# Do NOT modify the following line\n",
        "cat << EOF > /tmp/myscript.py\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from operator import add\n",
        "\n",
        "def main():\n",
        "    conf = SparkConf()\n",
        "    conf.set(\"spark.app.name\", \"My Python script\")\n",
        "\n",
        "    # Initialise the SparkContext\n",
        "    sc = SparkContext(conf=conf)\n",
        "    sc.setLogLevel(\"FATAL\")\n",
        "\n",
        "    rdd = sc.parallelize(range(100000)).cache()\n",
        "\n",
        "    rdd2 = rdd.map(lambda x: (x, 2*x))\\\n",
        "              .map(lambda (x,y): (x-100, y**2))\\\n",
        "              .reduceByKey(lambda x,y: x+y)\\\n",
        "              .values()\n",
        "\n",
        "    r = rdd2.reduce(add)\n",
        "\n",
        "    print(\"Final result = {0}\".format(r))\n",
        "\n",
        "    # Stop the SparkContext\n",
        "    sc.stop()\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "EOF"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-87f0c81908b6>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-87f0c81908b6>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    cat << EOF > /tmp/myscript.py\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLVf_t_RMnM6"
      },
      "source": [
        "!spark/bin/spark-submit --master local[8] \"$DRIVE_DATA\"myscript.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}